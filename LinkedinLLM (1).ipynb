{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch numpy matplotlib tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rxx_rmBBUHmT",
        "outputId": "a3f11b1a-0c20-4bdf-a543-4dead49cb363"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.5)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m105.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "o0R4R2prUFwi",
        "outputId": "43afa908-630f-4cfe-bb52-fa1e1aa0bc7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/100: 100%|██████████| 6/6 [00:01<00:00,  4.78it/s, loss=6.07, lr=1.93e-5]\n",
            "Epoch 2/100: 100%|██████████| 6/6 [00:00<00:00,  6.37it/s, loss=5.82, lr=4.04e-5]\n",
            "Epoch 3/100: 100%|██████████| 6/6 [00:00<00:00,  6.35it/s, loss=5.68, lr=7.32e-5]\n",
            "Epoch 4/100: 100%|██████████| 6/6 [00:00<00:00,  6.35it/s, loss=5.62, lr=0.000114]\n",
            "Epoch 5/100: 100%|██████████| 6/6 [00:00<00:00,  6.34it/s, loss=5.41, lr=0.00016]\n",
            "Epoch 6/100: 100%|██████████| 6/6 [00:00<00:00,  6.30it/s, loss=5.19, lr=0.000205]\n",
            "Epoch 7/100: 100%|██████████| 6/6 [00:00<00:00,  6.25it/s, loss=5.02, lr=0.000245]\n",
            "Epoch 8/100: 100%|██████████| 6/6 [00:00<00:00,  6.30it/s, loss=4.77, lr=0.000276]\n",
            "Epoch 9/100: 100%|██████████| 6/6 [00:00<00:00,  6.26it/s, loss=4.58, lr=0.000295]\n",
            "Epoch 10/100: 100%|██████████| 6/6 [00:00<00:00,  6.26it/s, loss=4.46, lr=0.0003]\n",
            "Epoch 11/100: 100%|██████████| 6/6 [00:00<00:00,  6.20it/s, loss=4.32, lr=0.0003]\n",
            "Epoch 12/100: 100%|██████████| 6/6 [00:00<00:00,  6.20it/s, loss=3.99, lr=0.0003]\n",
            "Epoch 13/100: 100%|██████████| 6/6 [00:00<00:00,  6.17it/s, loss=3.8, lr=0.000299]\n",
            "Epoch 14/100: 100%|██████████| 6/6 [00:00<00:00,  6.23it/s, loss=3.77, lr=0.000298]\n",
            "Epoch 15/100: 100%|██████████| 6/6 [00:00<00:00,  6.28it/s, loss=3.64, lr=0.000298]\n",
            "Epoch 16/100: 100%|██████████| 6/6 [00:00<00:00,  6.18it/s, loss=3.57, lr=0.000297]\n",
            "Epoch 17/100: 100%|██████████| 6/6 [00:00<00:00,  6.20it/s, loss=3.26, lr=0.000295]\n",
            "Epoch 18/100: 100%|██████████| 6/6 [00:00<00:00,  6.22it/s, loss=2.99, lr=0.000294]\n",
            "Epoch 19/100: 100%|██████████| 6/6 [00:00<00:00,  6.15it/s, loss=2.91, lr=0.000292]\n",
            "Epoch 20/100: 100%|██████████| 6/6 [00:00<00:00,  6.04it/s, loss=2.72, lr=0.000291]\n",
            "Epoch 21/100: 100%|██████████| 6/6 [00:00<00:00,  6.08it/s, loss=2.53, lr=0.000289]\n",
            "Epoch 22/100: 100%|██████████| 6/6 [00:00<00:00,  6.07it/s, loss=2.44, lr=0.000287]\n",
            "Epoch 23/100: 100%|██████████| 6/6 [00:00<00:00,  6.09it/s, loss=2.24, lr=0.000284]\n",
            "Epoch 24/100: 100%|██████████| 6/6 [00:00<00:00,  6.05it/s, loss=2.05, lr=0.000282]\n",
            "Epoch 25/100: 100%|██████████| 6/6 [00:00<00:00,  6.08it/s, loss=1.91, lr=0.000279]\n",
            "Epoch 26/100: 100%|██████████| 6/6 [00:00<00:00,  6.06it/s, loss=1.72, lr=0.000277]\n",
            "Epoch 27/100: 100%|██████████| 6/6 [00:00<00:00,  6.03it/s, loss=1.69, lr=0.000274]\n",
            "Epoch 28/100: 100%|██████████| 6/6 [00:01<00:00,  5.99it/s, loss=1.46, lr=0.000271]\n",
            "Epoch 29/100: 100%|██████████| 6/6 [00:01<00:00,  5.99it/s, loss=1.49, lr=0.000268]\n",
            "Epoch 30/100: 100%|██████████| 6/6 [00:01<00:00,  5.93it/s, loss=1.3, lr=0.000264]\n",
            "Epoch 31/100: 100%|██████████| 6/6 [00:01<00:00,  5.83it/s, loss=1.2, lr=0.000261]\n",
            "Epoch 32/100: 100%|██████████| 6/6 [00:01<00:00,  5.91it/s, loss=1.13, lr=0.000257]\n",
            "Epoch 33/100: 100%|██████████| 6/6 [00:01<00:00,  5.91it/s, loss=1.02, lr=0.000254]\n",
            "Epoch 34/100: 100%|██████████| 6/6 [00:01<00:00,  5.84it/s, loss=0.906, lr=0.00025]\n",
            "Epoch 35/100: 100%|██████████| 6/6 [00:01<00:00,  5.81it/s, loss=0.814, lr=0.000246]\n",
            "Epoch 36/100: 100%|██████████| 6/6 [00:01<00:00,  5.78it/s, loss=0.759, lr=0.000242]\n",
            "Epoch 37/100: 100%|██████████| 6/6 [00:01<00:00,  5.70it/s, loss=0.669, lr=0.000237]\n",
            "Epoch 38/100: 100%|██████████| 6/6 [00:01<00:00,  5.66it/s, loss=0.573, lr=0.000233]\n",
            "Epoch 39/100: 100%|██████████| 6/6 [00:01<00:00,  5.77it/s, loss=0.512, lr=0.000229]\n",
            "Epoch 40/100: 100%|██████████| 6/6 [00:01<00:00,  5.78it/s, loss=0.486, lr=0.000224]\n",
            "Epoch 41/100: 100%|██████████| 6/6 [00:01<00:00,  5.73it/s, loss=0.441, lr=0.00022]\n",
            "Epoch 42/100: 100%|██████████| 6/6 [00:01<00:00,  5.70it/s, loss=0.382, lr=0.000215]\n",
            "Epoch 43/100: 100%|██████████| 6/6 [00:01<00:00,  5.67it/s, loss=0.405, lr=0.00021]\n",
            "Epoch 44/100: 100%|██████████| 6/6 [00:01<00:00,  5.68it/s, loss=0.382, lr=0.000205]\n",
            "Epoch 45/100: 100%|██████████| 6/6 [00:01<00:00,  5.69it/s, loss=0.308, lr=0.0002]\n",
            "Epoch 46/100: 100%|██████████| 6/6 [00:01<00:00,  5.70it/s, loss=0.303, lr=0.000196]\n",
            "Epoch 47/100: 100%|██████████| 6/6 [00:01<00:00,  5.66it/s, loss=0.251, lr=0.000191]\n",
            "Epoch 48/100: 100%|██████████| 6/6 [00:01<00:00,  5.57it/s, loss=0.251, lr=0.000185]\n",
            "Epoch 49/100: 100%|██████████| 6/6 [00:01<00:00,  5.66it/s, loss=0.236, lr=0.00018]\n",
            "Epoch 50/100: 100%|██████████| 6/6 [00:01<00:00,  5.68it/s, loss=0.207, lr=0.000175]\n",
            "Epoch 51/100: 100%|██████████| 6/6 [00:01<00:00,  5.65it/s, loss=0.164, lr=0.00017]\n",
            "Epoch 52/100: 100%|██████████| 6/6 [00:01<00:00,  5.62it/s, loss=0.158, lr=0.000165]\n",
            "Epoch 53/100: 100%|██████████| 6/6 [00:01<00:00,  5.72it/s, loss=0.137, lr=0.00016]\n",
            "Epoch 54/100: 100%|██████████| 6/6 [00:01<00:00,  5.68it/s, loss=0.133, lr=0.000154]\n",
            "Epoch 55/100: 100%|██████████| 6/6 [00:01<00:00,  5.75it/s, loss=0.157, lr=0.000149]\n",
            "Epoch 56/100: 100%|██████████| 6/6 [00:01<00:00,  5.75it/s, loss=0.124, lr=0.000144]\n",
            "Epoch 57/100: 100%|██████████| 6/6 [00:01<00:00,  5.74it/s, loss=0.12, lr=0.000139]\n",
            "Epoch 58/100: 100%|██████████| 6/6 [00:01<00:00,  5.76it/s, loss=0.111, lr=0.000133]\n",
            "Epoch 59/100: 100%|██████████| 6/6 [00:01<00:00,  5.82it/s, loss=0.1, lr=0.000128]\n",
            "Epoch 60/100: 100%|██████████| 6/6 [00:01<00:00,  5.84it/s, loss=0.103, lr=0.000123]\n",
            "Epoch 61/100: 100%|██████████| 6/6 [00:01<00:00,  5.78it/s, loss=0.104, lr=0.000118]\n",
            "Epoch 62/100: 100%|██████████| 6/6 [00:01<00:00,  5.77it/s, loss=0.0969, lr=0.000113]\n",
            "Epoch 63/100: 100%|██████████| 6/6 [00:01<00:00,  5.79it/s, loss=0.0951, lr=0.000108]\n",
            "Epoch 64/100: 100%|██████████| 6/6 [00:01<00:00,  5.82it/s, loss=0.0842, lr=0.000103]\n",
            "Epoch 65/100: 100%|██████████| 6/6 [00:01<00:00,  5.94it/s, loss=0.0885, lr=9.79e-5]\n",
            "Epoch 66/100: 100%|██████████| 6/6 [00:01<00:00,  5.90it/s, loss=0.0788, lr=9.3e-5]\n",
            "Epoch 67/100: 100%|██████████| 6/6 [00:01<00:00,  5.94it/s, loss=0.0716, lr=8.82e-5]\n",
            "Epoch 68/100: 100%|██████████| 6/6 [00:01<00:00,  5.87it/s, loss=0.0821, lr=8.35e-5]\n",
            "Epoch 69/100: 100%|██████████| 6/6 [00:01<00:00,  5.92it/s, loss=0.0673, lr=7.88e-5]\n",
            "Epoch 70/100: 100%|██████████| 6/6 [00:01<00:00,  5.81it/s, loss=0.0732, lr=7.42e-5]\n",
            "Epoch 71/100: 100%|██████████| 6/6 [00:01<00:00,  5.91it/s, loss=0.0695, lr=6.98e-5]\n",
            "Epoch 72/100: 100%|██████████| 6/6 [00:01<00:00,  5.94it/s, loss=0.0645, lr=6.54e-5]\n",
            "Epoch 73/100: 100%|██████████| 6/6 [00:01<00:00,  5.94it/s, loss=0.0686, lr=6.11e-5]\n",
            "Epoch 74/100: 100%|██████████| 6/6 [00:01<00:00,  5.91it/s, loss=0.062, lr=5.7e-5]\n",
            "Epoch 75/100: 100%|██████████| 6/6 [00:01<00:00,  5.92it/s, loss=0.0585, lr=5.29e-5]\n",
            "Epoch 76/100: 100%|██████████| 6/6 [00:01<00:00,  5.95it/s, loss=0.0579, lr=4.9e-5]\n",
            "Epoch 77/100: 100%|██████████| 6/6 [00:01<00:00,  5.93it/s, loss=0.0564, lr=4.52e-5]\n",
            "Epoch 78/100: 100%|██████████| 6/6 [00:01<00:00,  5.91it/s, loss=0.0629, lr=4.15e-5]\n",
            "Epoch 79/100: 100%|██████████| 6/6 [00:01<00:00,  5.95it/s, loss=0.0607, lr=3.79e-5]\n",
            "Epoch 80/100: 100%|██████████| 6/6 [00:01<00:00,  5.88it/s, loss=0.0642, lr=3.45e-5]\n",
            "Epoch 81/100: 100%|██████████| 6/6 [00:01<00:00,  5.93it/s, loss=0.0583, lr=3.13e-5]\n",
            "Epoch 82/100: 100%|██████████| 6/6 [00:00<00:00,  6.02it/s, loss=0.0566, lr=2.81e-5]\n",
            "Epoch 83/100: 100%|██████████| 6/6 [00:01<00:00,  5.80it/s, loss=0.0571, lr=2.52e-5]\n",
            "Epoch 84/100: 100%|██████████| 6/6 [00:01<00:00,  5.86it/s, loss=0.0586, lr=2.23e-5]\n",
            "Epoch 85/100: 100%|██████████| 6/6 [00:01<00:00,  5.98it/s, loss=0.0641, lr=1.97e-5]\n",
            "Epoch 86/100: 100%|██████████| 6/6 [00:01<00:00,  5.93it/s, loss=0.0591, lr=1.72e-5]\n",
            "Epoch 87/100: 100%|██████████| 6/6 [00:01<00:00,  5.93it/s, loss=0.0559, lr=1.48e-5]\n",
            "Epoch 88/100: 100%|██████████| 6/6 [00:01<00:00,  5.91it/s, loss=0.059, lr=1.26e-5]\n",
            "Epoch 89/100: 100%|██████████| 6/6 [00:01<00:00,  5.94it/s, loss=0.0568, lr=1.06e-5]\n",
            "Epoch 90/100: 100%|██████████| 6/6 [00:01<00:00,  5.90it/s, loss=0.0651, lr=8.75e-6]\n",
            "Epoch 91/100: 100%|██████████| 6/6 [00:01<00:00,  5.94it/s, loss=0.0561, lr=7.08e-6]\n",
            "Epoch 92/100: 100%|██████████| 6/6 [00:01<00:00,  5.94it/s, loss=0.0468, lr=5.57e-6]\n",
            "Epoch 93/100: 100%|██████████| 6/6 [00:01<00:00,  5.90it/s, loss=0.0458, lr=4.25e-6]\n",
            "Epoch 94/100: 100%|██████████| 6/6 [00:01<00:00,  5.85it/s, loss=0.0553, lr=3.1e-6]\n",
            "Epoch 95/100: 100%|██████████| 6/6 [00:01<00:00,  5.89it/s, loss=0.0488, lr=2.13e-6]\n",
            "Epoch 96/100: 100%|██████████| 6/6 [00:01<00:00,  5.93it/s, loss=0.0557, lr=1.34e-6]\n",
            "Epoch 97/100: 100%|██████████| 6/6 [00:01<00:00,  5.90it/s, loss=0.0559, lr=7.34e-7]\n",
            "Epoch 98/100: 100%|██████████| 6/6 [00:01<00:00,  5.88it/s, loss=0.0502, lr=3.08e-7]\n",
            "Epoch 99/100: 100%|██████████| 6/6 [00:01<00:00,  5.85it/s, loss=0.0594, lr=6.47e-8]\n",
            "Epoch 100/100: 100%|██████████| 6/6 [00:01<00:00,  5.87it/s, loss=0.0515, lr=3.74e-9]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA9wAAAIjCAYAAADx4xNlAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAZdVJREFUeJzt3Xd4VAXexfFzp2RSSEJISCAQegm9N1FgBQRFFEFRwBVYy66Aq6K7VhRsWFZlbdhWsaGCL2BFBBUUBelI7x1CCCGdJJPMff9IgQhCgEzuTPL9PE+ezNy5M3Mm+Rk5c8sYpmmaAgAAAAAAZcpmdQAAAAAAACoiCjcAAAAAAF5A4QYAAAAAwAso3AAAAAAAeAGFGwAAAAAAL6BwAwAAAADgBRRuAAAAAAC8gMINAAAAAIAXULgBAAAAAPACCjcAoEIaNWqU6tWrd173nThxogzDKNtA8Bu9evVSr169rI4BAKgAKNwAgHJlGEapvhYuXGh1VEuMGjVKVapUsTpGqZimqQ8++EA9evRQ1apVFRwcrFatWumxxx5TZmam1fGK7d69u9Rzt3v3bqvjAgAqEMM0TdPqEACAyuPDDz8scf3999/X/Pnz9cEHH5RY3rdvX8XExJz387jdbnk8HrlcrnO+b15envLy8hQYGHjez3++Ro0apc8++0wZGRnl/tznIj8/X8OHD9eMGTN0ySWXaPDgwQoODtbPP/+s6dOnq3nz5lqwYMEF/Q7LSmZmpmbPnl1i2fPPP6/9+/frxRdfLLH8mmuukdPplCQFBASUW0YAQMVE4QYAWGrcuHF69dVXdbb/HWVlZSk4OLicUlnHXwr35MmT9eCDD+ree+/Vc889V+K2L7/8UoMGDdJll12muXPnlmuu0s7JlVdeqfXr17NFGwDgVexSDgDwOb169VLLli21cuVK9ejRQ8HBwXrwwQclSZ9//rkGDBig2NhYuVwuNWzYUI8//rjy8/NLPMYfj+Eu2q34P//5j9588001bNhQLpdLnTp10vLly0vc93THcBuGoXHjxmnOnDlq2bKlXC6XWrRooW+//faU/AsXLlTHjh0VGBiohg0b6o033ijz48JnzpypDh06KCgoSFFRUbrxxht14MCBEuskJCRo9OjRql27tlwul2rWrKmrr766RMlcsWKF+vXrp6ioKAUFBal+/fr629/+dsbnPn78uJ577jk1adJEkydPPuX2gQMHauTIkfr222+1dOlSSQUFt0GDBqd9vG7duqljx44lln344YfFr69atWq64YYbtG/fvhLrnGlOLsQfj+FeuHChDMPQjBkzNGnSJNWqVUuhoaG69tprlZqaqpycHN11112Kjo5WlSpVNHr0aOXk5JzyuKV5TQCAisVhdQAAAE7n6NGjuvzyy3XDDTfoxhtvLN41edq0aapSpYrGjx+vKlWq6IcfftAjjzyitLS0U7a0ns706dOVnp6uv//97zIMQ88++6wGDx6snTt3Fu9K/GcWL16sWbNmacyYMQoNDdVLL72kIUOGaO/evYqMjJQkrV69Wv3791fNmjU1adIk5efn67HHHlP16tUv/IdSaNq0aRo9erQ6deqkyZMn6/Dhw/rvf/+rX375RatXr1bVqlUlSUOGDNGGDRt0xx13qF69ekpMTNT8+fO1d+/e4uuXXXaZqlevrvvvv19Vq1bV7t27NWvWrLP+HI4dO6Y777xTDsfp/ylx00036d1339VXX32lrl276vrrr9dNN92k5cuXq1OnTsXr7dmzR0uXLi3xu3vyySc1YcIEDR06VLfccouOHDmil19+WT169Cjx+qQ/nxNvmDx5soKCgnT//fdr+/btevnll+V0OmWz2XTs2DFNnDhRS5cu1bRp01S/fn098sgj5/WaAAAViAkAgIXGjh1r/vF/Rz179jQlma+//vop62dlZZ2y7O9//7sZHBxsZmdnFy8bOXKkWbdu3eLru3btMiWZkZGRZnJycvHyzz//3JRkfvnll8XLHn300VMySTIDAgLM7du3Fy9bu3atKcl8+eWXi5cNHDjQDA4ONg8cOFC8bNu2babD4TjlMU9n5MiRZkhIyJ/enpuba0ZHR5stW7Y0jx8/Xrz8q6++MiWZjzzyiGmapnns2DFTkvncc8/96WPNnj3blGQuX778rLlONmXKFFOSOXv27D9dJzk52ZRkDh482DRN00xNTTVdLpd5zz33lFjv2WefNQ3DMPfs2WOapmnu3r3btNvt5pNPPllivXXr1pkOh6PE8jPNydkMGDCgxHycrGfPnmbPnj2Lr//444+mJLNly5Zmbm5u8fJhw4aZhmGYl19+eYn7d+vWrcRjn8trAgBULOxSDgDwSS6XS6NHjz5leVBQUPHl9PR0JSUl6ZJLLlFWVpY2b9581se9/vrrFRERUXz9kksukSTt3LnzrPft06ePGjZsWHy9devWCgsLK75vfn6+FixYoEGDBik2NrZ4vUaNGunyyy8/6+OXxooVK5SYmKgxY8aUOKnbgAEDFB8fr6+//lpSwc8pICBACxcu1LFjx077WEVbVb/66iu53e5SZ0hPT5ckhYaG/uk6RbelpaVJksLCwnT55ZdrxowZJY7X//TTT9W1a1fVqVNHkjRr1ix5PB4NHTpUSUlJxV81atRQ48aN9eOPP5Z4nj+bE2+46aabSuwF0aVLF5mmecou+F26dNG+ffuUl5cn6dxfEwCg4qBwAwB8Uq1atU57lugNGzbommuuUXh4uMLCwlS9enXdeOONkqTU1NSzPm5RsStSVL7/rJSe6b5F9y+6b2Jioo4fP65GjRqdst7plp2PPXv2SJKaNm16ym3x8fHFt7tcLj3zzDOaO3euYmJi1KNHDz377LNKSEgoXr9nz54aMmSIJk2apKioKF199dV69913T3v88cmKynRR8T6d05Xy66+/Xvv27dOSJUskSTt27NDKlSt1/fXXF6+zbds2maapxo0bq3r16iW+Nm3apMTExBLP82dz4g1//P2Hh4dLkuLi4k5Z7vF4iufxXF8TAKDi4BhuAIBPOnlLdpGUlBT17NlTYWFheuyxx9SwYUMFBgZq1apVuu++++TxeM76uHa7/bTLzVJ8aMeF3NcKd911lwYOHKg5c+Zo3rx5mjBhgiZPnqwffvhB7dq1k2EY+uyzz7R06VJ9+eWXmjdvnv72t7/p+eef19KlS//088CbNWsmSfr99981aNCg067z+++/S5KaN29evGzgwIEKDg7WjBkzdNFFF2nGjBmy2Wy67rrritfxeDwyDENz58497c/7j5lONyfe8me//7PNxbm+JgBAxUHhBgD4jYULF+ro0aOaNWuWevToUbx8165dFqY6ITo6WoGBgdq+ffspt51u2fmoW7euJGnLli269NJLS9y2ZcuW4tuLNGzYUPfcc4/uuecebdu2TW3bttXzzz9f4vPQu3btqq5du+rJJ5/U9OnTNWLECH3yySe65ZZbTpvh4osvVtWqVTV9+nQ99NBDpy2R77//vqSCs5MXCQkJ0ZVXXqmZM2fqhRde0KeffqpLLrmkxO73DRs2lGmaql+/vpo0aXKOPx3fVBFfEwCgdNilHADgN4qK3clblHNzc/Xaa69ZFakEu92uPn36aM6cOTp48GDx8u3bt5fZ51F37NhR0dHRev3110vs+j137lxt2rRJAwYMkFTwedTZ2dkl7tuwYUOFhoYW3+/YsWOnbJ1v27atJJ1xt/Lg4GDde++92rJlix566KFTbv/66681bdo09evXT127di1x2/XXX6+DBw/q7bff1tq1a0vsTi5JgwcPlt1u16RJk07JZpqmjh49+qe5fFVFfE0AgNJhCzcAwG9cdNFFioiI0MiRI/XPf/5ThmHogw8+8KlduidOnKjvvvtO3bt31+233678/Hy98soratmypdasWVOqx3C73XriiSdOWV6tWjWNGTNGzzzzjEaPHq2ePXtq2LBhxR8LVq9ePd19992SpK1bt6p3794aOnSomjdvLofDodmzZ+vw4cO64YYbJEnvvfeeXnvtNV1zzTVq2LCh0tPT9dZbbyksLExXXHHFGTPef//9Wr16tZ555hktWbJEQ4YMUVBQkBYvXqwPP/xQzZo103vvvXfK/a644gqFhobq3nvvld1u15AhQ0rc3rBhQz3xxBN64IEHtHv3bg0aNEihoaHatWuXZs+erdtuu0333ntvqX6OvqIiviYAQOlQuAEAfiMyMlJfffWV7rnnHj388MOKiIjQjTfeqN69e6tfv35Wx5MkdejQQXPnztW9996rCRMmKC4uTo899pg2bdpUqrOoSwVb7SdMmHDK8oYNG2rMmDEaNWqUgoOD9fTTT+u+++5TSEiIrrnmGj3zzDPFZx6Pi4vTsGHD9P333+uDDz6Qw+FQfHy8ZsyYUVxye/bsqWXLlumTTz7R4cOHFR4ers6dO+ujjz5S/fr1z5jRbrdrxowZev/99/X2229rwoQJys3NVcOGDfXoo4/qnnvuUUhIyCn3CwwM1FVXXaWPPvpIffr0UXR09Cnr3H///WrSpIlefPFFTZo0qfj1XHbZZbrqqqtK9TP0NRXxNQEAzs4wfWmzAAAAFdSgQYO0YcMGbdu2zeooAACgnHAMNwAAZez48eMlrm/btk3ffPONevXqZU0gAABgCbZwAwBQxmrWrKlRo0apQYMG2rNnj6ZOnaqcnBytXr1ajRs3tjoeAAAoJxzDDQBAGevfv78+/vhjJSQkyOVyqVu3bnrqqaco2wAAVDJs4QYAAAAAwAs4hhsAAAAAAC+gcAMAAAAA4AV+fQy3x+PRwYMHFRoaKsMwrI4DAAAAAKjgTNNUenq6YmNjZbOdeRu2XxfugwcPKi4uzuoYAAAAAIBKZt++fapdu/YZ1/Hrwh0aGiqp4IWGhYVZnObPud1ufffdd7rsssvkdDqtjgP8KWYV/oJZhb9gVuEvmFX4A1+Z07S0NMXFxRX30TPx68JdtBt5WFiYzxfu4OBghYWF8QcMPo1Zhb9gVuEvmFX4C2YV/sDX5rQ0hzVz0jQAAAAAALyAwg0AAAAAgBdQuAEAAAAA8AIKNwAAAAAAXkDhBgAAAADACyjcAAAAAAB4AYUbAAAAAAAvoHADAAAAAOAFFG4AAAAAALyAwg0AAAAAgBdQuAEAAAAA8AIKNwAAAAAAXkDhBgAAAADACyjcAAAAAAB4AYUbAAAAAAAvoHADAAAAAOAFFO5ykJ6dp/kHDGXm5FkdBQAAAABQTijc5eDm91fqq712TVuy1+ooAAAAAIByQuEuBzd1rSNJenvxbh3LzLU4DQAAAACgPFC4y8EVLWuoVrCpjJw8vb5oh9VxAAAAAADlgMJdDmw2QwPqeCRJ037drYTUbIsTAQAAAAC8jcJdTppXNdWxblXl5Hn00g/brI4DAAAAAPAyCnc5MQzpnr6NJUkzlu/T7qRMixMBAAAAALyJwl2OOtaN0F+aVleex9QL87daHQcAAAAA4EUU7nJ2b7+mkqQv1h7UxoNpFqcBAAAAAHiL5YX7wIEDuvHGGxUZGamgoCC1atVKK1assDqW17SIDdfANrGSpP98t8XiNAAAAAAAb7G0cB87dkzdu3eX0+nU3LlztXHjRj3//POKiIiwMpbXje/bRHaboR82J2rF7mSr4wAAAAAAvMBh5ZM/88wziouL07vvvlu8rH79+n+6fk5OjnJycoqvp6UV7JLtdrvldru9F/QCFWUr+l47PEDXto/VpysO6Om5mzT95k4yDMPKiICkU2cV8FXMKvwFswp/wazCH/jKnJ7L8xumaZpezHJGzZs3V79+/bR//34tWrRItWrV0pgxY3Trrbeedv2JEydq0qRJpyyfPn26goODvR23TKXkSI+vtivPNPT3+Hw1j7Ds1wAAAAAAKKWsrCwNHz5cqampCgsLO+O6lhbuwMBASdL48eN13XXXafny5brzzjv1+uuva+TIkaesf7ot3HFxcUpKSjrrC7WS2+3W/Pnz1bdvXzmdzuLlT3+7Rf/7ZY/ia4Tq89u7ymZjKzes9WezCvgaZhX+glmFv2BW4Q98ZU7T0tIUFRVVqsJt6S7lHo9HHTt21FNPPSVJateundavX/+nhdvlcsnlcp2y3Ol0+sUfhj/mHHtpE3264oA2J6Rr3uYkXVV4MjXAav7y3xTArMJfMKvwF8wq/IHVc3ouz23pSdNq1qyp5s2bl1jWrFkz7d2716JE5ataSIBuvaSBJOmF77bIne+xOBEAAAAAoKxYWri7d++uLVtKfjTW1q1bVbduXYsSlb+bL6mvyJAA7T6apZkr9lsdBwAAAABQRiwt3HfffbeWLl2qp556Stu3b9f06dP15ptvauzYsVbGKldVXA6N+UsjSdJ/v9+q9GzODAkAAAAAFYGlhbtTp06aPXu2Pv74Y7Vs2VKPP/64pkyZohEjRlgZq9yN6FJHdaoF63Bajv4zb8vZ7wAAAAAA8HmWFm5JuvLKK7Vu3TplZ2dr06ZNf/qRYBVZoNOup65pJUl6f+kerdyTbHEiAAAAAMCFsrxwo8DFjaN0bYfaMk3p/v9bp5y8fKsjAQAAAAAuAIXbhzx0RTNFhgRoW2KGXl+40+o4AAAAAIALQOH2IREhAXr0qhaSpFd+3KZth9MtTgQAAAAAOF8Ubh8zsHVNXRofLXe+qftnrZPHY1odCQAAAABwHijcPsYwDD0+qKVCAuxaueeYPvptj9WRAAAAAADngcLtg2pVDdK/+8dLkp75dosOpR63OBEAAAAA4FxRuH3UjV3rqn2dqsrIydOEOetlmuxaDgAAAAD+hMLto+w2Q08PaS2n3dCCTYn6Zl2C1ZEAAAAAAOeAwu3DmsSE6vZejSRJj36xQalZbosTAQAAAABKi8Lt48b+paEaVg9RUkaOnvpmk9VxAAAAAAClROH2cS6HXc8MaS1J+nTFPv24OdHiRAAAAACA0qBw+4GO9arpr13rSpL+8eFKLdxC6QYAAAAAX0fh9hMPX9lMfZpFKyfPo9veX6n5Gw9bHQkAAAAAcAYUbj/hctj12ogOurxlDeXme3T7hyv1zbpDVscCAAAAAPwJCrcfCXDY9PKwdrq6bazyPKbGTV+lOasPWB0LAAAAAHAaFG4/47Db9MLQtrquQ215TOnuGWs0Y/k+q2MBAAAAAP6Awu2H7DZDzwxprRFd6sg0pX//3+/6YOkeq2MBAAAAAE5C4fZTNpuhJwa11Oju9SRJE+as19s/77Q2FAAAAACgGIXbjxmGoUeubK5/9GwoSXri6016beF2i1MBAAAAACQKt98zDEP39W+qO3s3liQ9++0WTf9tr8WpAAAAAAAU7grAMAzd3beJ/nlpI0nShM/X68fNiRanAgAAAIDKjcJdgdzdt4mu7VBb+R5TY6ev0rr9qVZHAgAAAIBKi8JdgRiGocmDW+mSxlHKys3X6GnLtS85y+pYAAAAAFApUbgrGKfdptdGtFezmmFKysjRyHeXKSUr1+pYAAAAAFDpULgroNBAp94d1Uk1wwO180imbnlvhbLd+VbHAgAAAIBKhcJdQdUID9S00Z0VGujQij3HdM+MtfJ4TKtjAQAAAEClQeGuwJrWCNUbf+0gp93Q1+sO6alvNlkdCQAAAAAqDQp3BXdRwyj957o2kqS3F+/Su7/ssjgRAAAAAFQOFO5K4Oq2tfTv/k0lSY99tVELNh62OBEAAAAAVHwU7kri9p4NNaJLHZmmdP+s3zlzOQAAAAB4GYW7kjAMQ48MbK7G0VWUlJGrJ7/meG4AAAAA8CYKdyXictj19JDWMgxp5sr9WrwtyepIAAAAAFBhUbgrmQ51I3RT17qSpAdnr9PxXD6fGwAAAAC8gcJdCf2rf7xqhgdqb3KWpizYanUcAAAAAKiQKNyVUBWXQ08MailJeuvnnVp/INXiRAAAAABQ8VC4K6nezWI0sE2sPKb0789+lzvfY3UkAAAAAKhQKNyV2KMDm6tqsFMbD6Xpf4t3WR0HAAAAACoUCnclFlXFpYcHNJckvTh/q3YnZVqcCAAAAAAqDgp3JTekfS1d3ChKOXkePTBrnUzTtDoSAAAAAFQIFO5KzjAMPXVNKwU6bVqy86hmrthvdSQAAAAAqBAo3FCdyGDd07epJOmJrzcqMT3b4kQAAAAA4P8o3JAkje5eT61qhSstO08Tv9hgdRwAAAAA8HsUbkiSHHabnh7SSnaboW/WJWjuukNWRwIAAAAAv0bhRrEWseH6R88GkqSH56zX0YwcixMBAAAAgP+icKOEf/ZurKYxoTqamatHPmfXcgAAAAA4XxRulOBy2PX80Day2wx9ve6Qvvr9oNWRAAAAAMAvUbhxipa1wjX2L40kSRPmrNeRdHYtBwAAAIBzReHGaY37SyM1qxmmY1luPTxnnUzTtDoSAAAAAPgVCjdOK8Bh0/PXtZHDZmjehsP6Yi27lgMAAADAuaBw4081jw3THZc2liQ98vkGJaZlW5wIAAAAAPwHhRtnNOYvDdUiNkypx916cDa7lgMAAABAaVG4cUZOu03PD20jp93Qgk2Jmr36gNWRAAAAAMAvULhxVvE1wnRXnyaSpIlfbNBhdi0HAAAAgLOicKNU/t6jgVrXDldadp4emMWu5QAAAABwNhRulIrDXnDW8gC7TT9sTtRnK/dbHQkAAAAAfBqFG6XWOCZUd/ct2LX8qW82KSUr1+JEAAAAAOC7KNw4J7deUl9NY0J1LMut5+ZtsToOAAAAAPgsSwv3xIkTZRhGia/4+HgrI+EsHHabJl3dQpI0fdlerdufanEiAAAAAPBNlm/hbtGihQ4dOlT8tXjxYqsj4Sy6NojUVW1iZZrSI1+sl8fDCdQAAAAA4I8sL9wOh0M1atQo/oqKirI6EkrhoQHNFBJg1+q9KfpsFSdQAwAAAIA/clgdYNu2bYqNjVVgYKC6deumyZMnq06dOqddNycnRzk5OcXX09LSJElut1tut7tc8p6Pomy+nPFcVQuya9xfGuqZeVv19NxNurRJpMKDnFbHwgWqiLOKiolZhb9gVuEvmFX4A1+Z03N5fsO08AOV586dq4yMDDVt2lSHDh3SpEmTdODAAa1fv16hoaGnrD9x4kRNmjTplOXTp09XcHBweUTGSfI90jO/23X4uKFLanh0bX2P1ZEAAAAAwKuysrI0fPhwpaamKiws7IzrWlq4/yglJUV169bVCy+8oJtvvvmU20+3hTsuLk5JSUlnfaFWcrvdmj9/vvr27Suns2JtBf51x1GNnLZSNkOafXtXNa/pu78HnF1FnlVULMwq/AWzCn/BrMIf+MqcpqWlKSoqqlSF2/Jdyk9WtWpVNWnSRNu3bz/t7S6XSy6X65TlTqfTL/4w+EvOc9EzvoYGtKqpr9cd0uNfb9HMf3STYRhWx8IFqoizioqJWYW/YFbhL5hV+AOr5/Rcntvyk6adLCMjQzt27FDNmjWtjoJz8NCAZgpy2rVizzHNXn3A6jgAAAAA4BMsLdz33nuvFi1apN27d+vXX3/VNddcI7vdrmHDhlkZC+cotmqQ7ujdSJL01DeblZbNyTYAAAAAwNLCvX//fg0bNkxNmzbV0KFDFRkZqaVLl6p69epWxsJ5uPni+moQFaKkjBxNmb/N6jgAAAAAYDlLj+H+5JNPrHx6lCGXw65Hr2qhke8s03tLdmtop9qKr8EJ1AAAAABUXj51DDf8W88m1dWvRYzyPaYe/XyDfOgE+AAAAABQ7ijcKFMTrmyuQKdNv+1K1hdrD1odBwAAAAAsQ+FGmaodEawxvQpOoDb5m83Kys2zOBEAAAAAWIPCjTJ3W48GiqsWpIS0bL364+k/Ux0AAAAAKjoKN8pcoNOuhwc0lyS99dMu7TmaaXEiAAAAACh/FG54xWXNY3RJ4yjl5nv0+FebrI4DAAAAAOWOwg2vMAxDjw5sLofN0IJNh7VwS6LVkQAAAACgXFG44TWNokM16qJ6kqTHvtyo3DyPtYEAAAAAoBxRuOFV/+zTWFFVXNqZlKlpv+6yOg4AAAAAlBsKN7wqLNCp+/o3lST9d8E2JaZlW5wIAAAAAMoHhRteN6R9bbWNq6rM3Hw9/e1mq+MAAAAAQLmgcMPrbDZDk65qIUmateqAVu5JtjgRAAAAAHgfhRvlok1cVQ3tWFuSNPGLjcr3mBYnAgAAAADvonCj3PyrX7xCXQ6tO5CqmSv2WR0HAAAAALyKwo1yUz3Upbv6NpEkPTtvi1Kz3BYnAgAAAADvoXCjXN3Ura4aR1dRcmauXpi/xeo4AAAAAOA1FG6UK6fdpomFJ1B7f+keLd151OJEAAAAAOAdFG6Uu+6NonRDpziZpvSvz9YqIyfP6kgAAAAAUOYo3LDEQwOaqVbVIO1LPq6nvtlkdRwAAAAAKHMUblgiNNCp565rLUma/tteLdp6xOJEAAAAAFC2KNywzEUNozTqonqSpPs++12pxzlrOQAAAICKg8INS93XP171o0KUkJatSV9usDoOAAAAAJQZCjcsFRRg13+uay2bIc1adUDfbUiwOhIAAAAAlAkKNyzXoW413dqjgSTpwdnrlJyZa3EiAAAAALhwFG74hLv7NFGTmCpKysjVhDnrrY4DAAAAABeMwg2fEOi06/nr2sphM/T1ukP6cu1BqyMBAAAAwAWhcMNntKodrrF/aSRJmvD5eiWmZ1ucCAAAAADOH4UbPmXcpY3UIjZMKVluPfB/62SaptWRAAAAAOC8ULjhU5x2m14Y2lYBdpu+35yoj37ba3UkAAAAADgvFG74nKY1QvWvfk0lSY99tVEbDqZanAgAAAAAzh2FGz7p5ovrq3d8tHLzPBo3fbXSs91WRwIAAACAc0Lhhk+y2Qz957o2ig0P1K6kTD0wi+O5AQAAAPgXCjd8VkRIgF4e3l4Om6Gvfj+k6cs4nhsAAACA/6Bww6d1qBuhf/cvOJ570pcczw0AAADAf1C44fNuubgBx3MDAAAA8DsUbvg8jucGAAAA4I8o3PALHM8NAAAAwN9QuOE3OtSN0H394yVxPDcAAAAA30fhhl+55ZL66tOM47kBAAAA+D4KN/yKYRQcz12rapB2JWXq4TnrrY4EAAAAAKdF4YbfqRocoJeGtZPdZujzNQf17foEqyMBAAAAwCko3PBLHepG6B89G0iSHp6zXilZuRYnAgAAAICSKNzwW3dc2liNoqsoKSNHj3250eo4AAAAAFAChRt+K9Bp13PXtpbNkGatPqAfNh+2OhIAAAAAFKNww6+1qxOhmy+uL0l6cNZ6pXHWcgAAAAA+gsINv3fPZU1VPypECWnZeurrTVbHAQAAAABJFG5UAIFOu54Z0lqGIX2yfJ9+3nbE6kgAAAAAQOFGxdC5fjWN7FZPknT//61TRk6etYEAAAAAVHoUblQY/+7fVHHVgnQg5biembvZ6jgAAAAAKjkKNyqM4ACHnhncWpL0wdI9WrLjqMWJAAAAAFRmFG5UKBc1itLwLnUkSff93+/KymXXcgAAAADWoHCjwnng8njFhgdqb3KW/jNvq9VxAAAAAFRSFG5UOKGBTj01uJUk6d1fd2nZrmSLEwEAAACojCjcqJB6NY3WdR1qyzSlOz9ZrWOZuVZHAgAAAFDJULhRYT16VQs1iArRodRs/euztTJN0+pIAAAAACoRCjcqrCouh14e3k4BDpsWbErUO7/stjoSAAAAgEqEwo0KrUVsuCYMaCZJenruJq3dl2JtIAAAAACVBoUbFd6NXevq8pY15M43Ne7jVUrLdlsdCQAAAEAl4DOF++mnn5ZhGLrrrrusjoIKxjAMPT2ktWpHBGlf8nE9MGsdx3MDAAAA8DqfKNzLly/XG2+8odatW1sdBRVUeJBTLw9rJ4fN0Ne/H9L0ZXutjgQAAACggrO8cGdkZGjEiBF66623FBERYXUcVGDt6kTo3/2bSpImfblRmw6lWZwIAAAAQEXmsDrA2LFjNWDAAPXp00dPPPHEGdfNyclRTk5O8fW0tILC5Ha75Xb77nG5Rdl8OWNlMbJLnH7ZnqRFW5M09qNVmvWPLgpxWf6fgc9gVuEvmFX4C2YV/oJZhT/wlTk9l+c3TAsPZv3kk0/05JNPavny5QoMDFSvXr3Utm1bTZky5bTrT5w4UZMmTTpl+fTp0xUcHOzltKgoMtzSs2vtSnUb6lzdoxGNPFZHAgAAAOAnsrKyNHz4cKWmpiosLOyM61pWuPft26eOHTtq/vz5xcdun61wn24Ld1xcnJKSks76Qq3kdrs1f/589e3bV06n0+o4kLRsd7L++s4KeUzpmcEtNLhdLasj+QRmFf6CWYW/YFbhL5hV+ANfmdO0tDRFRUWVqnBbti/typUrlZiYqPbt2xcvy8/P108//aRXXnlFOTk5stvtJe7jcrnkcrlOeSyn0+kXfxj8JWdl0L1xjO7q00QvzN+qCV9sUp3IUHVrGGl1LJ/BrMJfMKvwF8wq/AWzCn9g9Zyey3NbdtK03r17a926dVqzZk3xV8eOHTVixAitWbPmlLINlLWxf2mkPs1ilJvn0S3vLdfafSlWRwIAAABQgVhWuENDQ9WyZcsSXyEhIYqMjFTLli2tioVKxG4z9MrwdurWIFKZufka+e4ybT2cbnUsAAAAABWE5R8LBlgp0GnXWyM7qk1cVaVkuXXj279p79Esq2MBAAAAqAB8qnAvXLjwT0+YBnhLFZdD743upKYxoUpMz9GN//tNh9OyrY4FAAAAwM/5VOEGrFI1OEAf3NxZdaoFa29ylv76v990LDPX6lgAAAAA/BiFGygUHRaoj27popgwl7YeztCod5cpIyfP6lgAAAAA/BSFGzhJXLVgfXhzF0UEO7V2f6pufW+Fst35VscCAAAA4Ico3MAfNI4J1Xt/66wqLoeW7DyqcdNXy53vsToWAAAAAD9D4QZOo3Xtqnp7ZEe5HDYt2HRYk7/ZbHUkAAAAAH6Gwg38ia4NIvXysHaSpHd+2aUFGw9bnAgAAACAP6FwA2dwWYsauuXi+pKkez9bq4Mpxy1OBAAAAMBfULiBs/h3/3i1rh2ulCy37vxktfI4nhsAAABAKVC4gbMIcNj08rB2quJyaPnuY/rv99usjgQAAADAD1C4gVKoGxmiyYNbSZJe+XG7ftmeZHEiAAAAAL6Owg2U0sA2sRrWOU6mKd316RolZeRYHQkAAACAD6NwA+fgkStbqElMFR1Jz9H4GWvl8ZhWRwIAAADgoyjcwDkICrDrleHtFei06aetR/TmzzutjgQAAADAR1G4gXPUJCZUEwe2kCT9Z94Wrdp7zOJEAAAAAHwRhRs4D9d3itPANrHK85i6Y/pqpWa5rY4EAAAAwMdQuIHzYBiGnrqmpepUC9aBlOP69/+tlWlyPDcAAACAEyjcwHkKDXTqleHt5LQbmrfhsKYs4PO5AQAAAJxA4QYuQOvaVfXEoJaSpP9+v02frzlgcSIAAAAAvoLCDVyg6zvV0d97NJAk/Wvm71q5J9niRAAAAAB8AYUbKAP39Y/XZc1jlJvv0W3vr9S+5CyrIwEAAACwGIUbKAM2m6EpN7RVi9gwHc3M1d+mLVdaNmcuBwAAACozCjdQRoIDHHp7ZEfFhLm0LTFDd0xfrbx8j9WxAAAAAFiEwg2UoZrhQXr7pk4KdNq0aOsRPf7VRqsjAQAAALAIhRsoY61qh2vK9W0lSe8t2aP3ft1taR4AAAAA1qBwA17Qv2VN/bt/U0nSpC83aOGWRIsTAQAAAChvFG7AS27v2VDXdqgtjymNm75aWw+nWx0JAAAAQDmicANeYhiGnrqmlTrXr6aMnDz98+PVys3jJGoAAABAZUHhBrwowGHTq8PbKyLYqc0J6Xr5h21WRwIAAABQTijcgJdVD3XpiUGtJEmvLdyhtftSrA0EAAAAoFxQuIFyMKB1TV3ZuqbyPabumblW2e58qyMBAAAA8DIKN1BOHr+6paKquLQ9MUMvzt9qdRwAAAAAXkbhBspJREiAJg8u2LX8zZ93auWeZIsTAQAAAPAmCjdQjvo2j9Hg9rVkmtI9M9YqKzfP6kgAAAAAvITCDZSzRwe2UI2wQO0+mqVnv91idRwAAAAAXkLhBspZeJBTz1zbWpI07dfd+nVHksWJAAAAAHgDhRuwQM8m1TWscx1J0r8/+10ZOexaDgAAAFQ0FG7AIg8NaKbaEUHaf+y4nvx6k9VxAAAAAJQxCjdgkSouh54t3LX842V7tWjrEYsTAQAAAChLFG7AQhc1jNKoi+pJku5j13IAAACgQqFwAxa7r3+86kYGKyEtW6/8sN3qOAAAAADKCIUbsFhQgF0TBjSXJL2zeJd2J2VanAgAAABAWaBwAz6gd7No9WhSXbn5Hj3x9Uar4wAAAAAoAxRuwAcYhqFHrmwuh83Qgk2JnEANAAAAqADOq3Dv27dP+/fvL76+bNky3XXXXXrzzTfLLBhQ2TSKrqKRhSdQe+zLDXLne6wNBAAAAOCCnFfhHj58uH788UdJUkJCgvr27atly5bpoYce0mOPPVamAYHK5J+9GysyJEA7jmTqvV93Wx0HAAAAwAU4r8K9fv16de7cWZI0Y8YMtWzZUr/++qs++ugjTZs2rSzzAZVKeJBT9/ZrKkn674JtSsrIsTgRAAAAgPN1XoXb7XbL5XJJkhYsWKCrrrpKkhQfH69Dhw6VXTqgEhraMU4tYsOUnpOn/8zbYnUcAAAAAOfpvAp3ixYt9Prrr+vnn3/W/Pnz1b9/f0nSwYMHFRkZWaYBgcrGbjM08aoWkqRPV+zTuv2pFicCAAAAcD7Oq3A/88wzeuONN9SrVy8NGzZMbdq0kSR98cUXxbuaAzh/nepV01VtYmWa0qQvN8g0TasjAQAAADhHjvO5U69evZSUlKS0tDRFREQUL7/tttsUHBxcZuGAyuyBK+I1f+NhrdhzTF+sPair29ayOhIAAACAc3BeW7iPHz+unJyc4rK9Z88eTZkyRVu2bFF0dHSZBgQqq5rhQRrTq6EkafI3m5WVm2dxIgAAAADn4rwK99VXX633339fkpSSkqIuXbro+eef16BBgzR16tQyDQhUZrf2aKDaEUFKSMvW1IU7rI4DAAAA4BycV+FetWqVLrnkEknSZ599ppiYGO3Zs0fvv/++XnrppTINCFRmgU67Hh7QTJL0xk87tS85y+JEAAAAAErrvAp3VlaWQkNDJUnfffedBg8eLJvNpq5du2rPnj1lGhCo7Pq1qKGLGkYqN8+jRz5fzwnUAAAAAD9xXoW7UaNGmjNnjvbt26d58+bpsssukyQlJiYqLCysTAMClZ1hGHrs6hYKsNv045Yjmrliv9WRAAAAAJTCeRXuRx55RPfee6/q1aunzp07q1u3bpIKtna3a9euTAMCkBpFh2r8ZU0kSY99tVEHUo5bnAgAAADA2ZxX4b722mu1d+9erVixQvPmzSte3rt3b7344otlFg7ACbde0kDt61RVRk6e7vvsd3YtBwAAAHzceRVuSapRo4batWungwcPav/+gl1cO3furPj4+DILB+AEu83Qf65rI5fDpsXbk/TRb3utjgQAAADgDM6rcHs8Hj322GMKDw9X3bp1VbduXVWtWlWPP/64PB5PqR9n6tSpat26tcLCwhQWFqZu3bpp7ty55xMJqBQaVK+if/cveFPrqW82ae9RzloOAAAA+KrzKtwPPfSQXnnlFT399NNavXq1Vq9eraeeekovv/yyJkyYUOrHqV27tp5++mmtXLlSK1as0KWXXqqrr75aGzZsOJ9YQKUw+qJ66ly/mrJy8/Wvz9bK42HXcgAAAMAXnVfhfu+99/T222/r9ttvV+vWrdW6dWuNGTNGb731lqZNm1bqxxk4cKCuuOIKNW7cWE2aNNGTTz6pKlWqaOnSpecTC6gUbDZD/7m2jYID7PptV7Km/brb6kgAAAAATsNxPndKTk4+7bHa8fHxSk5OPq8g+fn5mjlzpjIzM4vPev5HOTk5ysnJKb6elpYmSXK73XK73ef1vOWhKJsvZ4R/qRnm1L/7NdHELzfp2XmbdXHDCNWPCrngx2VW4S+YVfgLZhX+glmFP/CVOT2X5zfM8zjVcZcuXdSlSxe99NJLJZbfcccdWrZsmX777bdSP9a6devUrVs3ZWdnq0qVKpo+fbquuOKK0647ceJETZo06ZTl06dPV3Bw8Lm9CMDPmab02iabtqbaVK+KqTtb5stmWJ0KAAAAqNiysrI0fPhwpaamKiws7IzrnlfhXrRokQYMGKA6deoUb41esmSJ9u3bp2+++UaXXHJJqR8rNzdXe/fuVWpqqj777DO9/fbbWrRokZo3b37Kuqfbwh0XF6ekpKSzvlArud1uzZ8/X3379pXT6bQ6DiqQgynHdcUrvyozJ1//7tdYt15c/4Iej1mFv2BW4S+YVfgLZhX+wFfmNC0tTVFRUaUq3Oe1S3nPnj21detWvfrqq9q8ebMkafDgwbrtttv0xBNPnFPhDggIUKNGjSRJHTp00PLly/Xf//5Xb7zxxinrulwuuVyuU5Y7nU6/+MPgLznhP+pWd+qRK5vrvv9bpynf71Df5jXVOCb0gh+XWYW/YFbhL5hV+AtmFf7A6jk9l+c+r8ItSbGxsXryySdLLFu7dq3+97//6c033zzfh5XH4ymxFRvAmQ3tGKdv1yfoxy1HdM/MtZp1+0Vy2M/rfIgAAAAAypCl/yp/4IEH9NNPP2n37t1at26dHnjgAS1cuFAjRoywMhbgVwzD0NNDWiss0KHf96dq6sIdVkcCAAAAIIsLd2Jiom666SY1bdpUvXv31vLlyzVv3jz17dvXyliA34kJC9Skq1tIkl76YZs2HEy1OBEAAACA896lvCz873//s/LpgQplUNta+nZ9guZtOKx7ZqzVF+MuVoCDXcsBAAAAq5xT4R48ePAZb09JSbmQLAAugGEYevKaVlq++5g2J6Trpe+36d5+Ta2OBQAAAFRa57T5Kzw8/IxfdevW1U033eStrADOIqqKS08MailJmrpoh9bsS7E2EAAAAFCJndMW7nfffddbOQCUkSta1dRVbWL1xdqDumfGGn39z0sU6LRbHQsAAACodDjAE6iAHru6haqHurTjSKae/26L1XEAAACASonCDVRAVYMD9PTgVpKktxfv0rJdyRYnAgAAACofCjdQQfVuFqPrOtSWaUr3zlyrzJw8qyMBAAAAlQqFG6jAJgxsrtjwQO1NztLTczdbHQcAAACoVCjcQAUWFujUs9e2kSR9sHSPftmeZHEiAAAAoPKgcAMV3MWNo/TXrnUlSf/+7HelZbstTgQAAABUDhRuoBK4//J41akWrAMpx/XYlxutjgMAAABUChRuoBIIcTn0/NA2Mgzps5X79c26Q1ZHAgAAACo8CjdQSXSqV01jejWUJD0wa50OpR63OBEAAABQsVG4gUrkrj5N1Lp2uFKPuzX+07XyeEyrIwEAAAAVFoUbqEScdpv+e0M7BTntWrLzqN78eafVkQAAAIAKi8INVDL1o0I08armkqTnv9uidftTLU4EAAAAVEwUbqASGtoxTv1b1JA739Sdn65WVm6e1ZEAAACACofCDVRChmFo8uBWiglzaeeRTD3x9SarIwEAAAAVDoUbqKQiQgL0wtC2kqTpv+3VdxsSrA0EAAAAVDAUbqAS694oSrf1aCBJuu//fldieo7FiQAAAICKg8INVHL3XNZEzWuG6ViWW/fNWi8+KQwAAAAoGxRuoJJzOex6aVhbuRw2Ld5+VD8lGFZHAgAAACoECjcANYoO1cMDmkmSvthj07oDfFQYAAAAcKEo3AAkSTd2rave8dWVbxq6ffoaJaZnWx0JAAAA8GsUbgCSCj4q7LkhLRUdaOpwWo5u/3CVcvLyrY4FAAAA+C0KN4BioYFO3Rqfr9BAh1buOaZH5myQaXIWNQAAAOB8ULgBlBAdJE0Z2lo2Q/p0xT699+tuqyMBAAAAfonCDeAUPRpH6f7L4yVJj3+9Sb9sT7I4EQAAAOB/KNwATuvWSxromna1lO8xNXb6Ku09mmV1JAAAAMCvULgBnJZhGJo8uJXa1A5XSpZbt7y/XBk5eVbHAgAAAPwGhRvAnwp02vXGXzsqOtSlrYczdPena+TxcBI1AAAAoDQo3ADOqEZ4oF7/awcF2G2av/GwpizYanUkAAAAwC9QuAGcVfs6EXrympaSpJd+2K5v1h2yOBEAAADg+yjcAErluo5x+lv3+pKke2as1aZDaRYnAgAAAHwbhRtAqT14Rby6N4rUcXe+bvtghVKycq2OBAAAAPgsCjeAUnPYbXplWHvVjgjSvuTjuuPj1crL91gdCwAAAPBJFG4A5yQiJEBv/rWjgpx2/bwtSc/N22J1JAAAAMAnUbgBnLPmsWF67rrWkqQ3ftqpL9YetDgRAAAA4Hso3ADOy5WtY/WPng0lSf/+bK02HEy1OBEAAADgWyjcAM7bv/o1VY8m1ZXt9ui291cqOZOTqAEAAABFKNwAzpvdZujlG9qpbmSwDqQc17jpqziJGgAAAFCIwg3ggoQHO/XmXzsqOMCuX3cc1eS5m62OBAAAAPgECjeAC9a0RqheGNpGkvS/xbs0a9V+ixMBAAAA1qNwAygT/VvW1B2XNpIkPTBrndYf4CRqAAAAqNwo3ADKzN19mujS+Gjl5Hk05qNVSj3utjoSAAAAYBkKN4AyY7MZenFoW9WOCNLe5Cz9a+ZamaZpdSwAAADAEhRuAGUqPNip10a0V4Ddpu82HtbbP++yOhIAAABgCQo3gDLXunZVTRjYXJL09LebtXx3ssWJAAAAgPJH4QbgFTd2qaOr2sQq32Nq3PRVSsrIsToSAAAAUK4o3AC8wjAMTR7cSg2rh+hwWo7u+mSN8j0czw0AAIDKg8INwGtCXA5NvbGDgpx2Ld6epJe+32Z1JAAAAKDcULgBeFWTmFA9NbilJOmlH7bpp61HLE4EAAAAlA8KNwCvu6ZdbQ3rXEemKd316RodSj1udSQAAADA6yjcAMrFowObq0VsmJIzczX2o1Vy53usjgQAAAB4FYUbQLkIdNo1dUQHhQY6tGpvip6eu9nqSAAAAIBXUbgBlJs6kcH6z3VtJEn/W7xL321IsDgRAAAA4D0UbgDlql+LGrrl4vqSpHtnrtW+5CyLEwEAAADeQeEGUO7+3T9ebeKqKi07T3d8vFq5eRzPDQAAgIqHwg2g3AU4bHplWDuFBTq0Zl+KnpvH8dwAAACoeCwt3JMnT1anTp0UGhqq6OhoDRo0SFu2bLEyEoByElctWM8VHs/91s+7NH/jYYsTAQAAAGXL0sK9aNEijR07VkuXLtX8+fPldrt12WWXKTMz08pYAMpJvxY19LfuJ47n3n+M47kBAABQcTisfPJvv/22xPVp06YpOjpaK1euVI8ePSxKBaA83X95vFbuSdba/am64+PVmvH3bnLaOdoFAAAA/s/Swv1HqampkqRq1aqd9vacnBzl5OQUX09LS5Mkud1uud1u7wc8T0XZfDkjIFkzq4akF4e20tWvLdXqvSl6+puNur9/03J7fvgn/q7CXzCr8BfMKvyBr8zpuTy/YZqm6cUspebxeHTVVVcpJSVFixcvPu06EydO1KRJk05ZPn36dAUHB3s7IgAvWnvU0Dtb7ZKkW+Pz1TLCJ/40AQAAACVkZWVp+PDhSk1NVVhY2BnX9ZnCffvtt2vu3LlavHixateufdp1TreFOy4uTklJSWd9oVZyu92aP3+++vbtK6fTaXUc4E9ZPatPfLNZ7y3Zq6pBTn0+pqtiqwaVewb4B6tnFSgtZhX+glmFP/CVOU1LS1NUVFSpCrdP7FI+btw4ffXVV/rpp5/+tGxLksvlksvlOmW50+n0iz8M/pITsGpWHxrQQqv3per3/am6e+Y6fcrx3DgL/q7CXzCr8BfMKvyB1XN6Ls9t6b9kTdPUuHHjNHv2bP3www+qX7++lXEAWKzg87nbKzTQoVV7U/TUN5usjgQAAACcN0sL99ixY/Xhhx9q+vTpCg0NVUJCghISEnT8+HErYwGwUJ3IYP2n8PO53/1lt2at2m9xIgAAAOD8WFq4p06dqtTUVPXq1Us1a9Ys/vr000+tjAXAYv1a1NAdlzaSJD0wa53WH0i1OBEAAABw7izfpfx0X6NGjbIyFgAfcHefJro0Plo5eR79/YOVOpqRc/Y7AQAAAD6EsxEB8Ek2m6EXr2+r+lEhOpByXGOnr5I732N1LAAAAKDUKNwAfFZ4kFNv/rWDQgLsWrozmZOoAQAAwK9QuAH4tMYxoXrh+raSOIkaAAAA/AuFG4DP69eihv550knU1u3nJGoAAADwfRRuAH7hrj5N1Lv4JGorlMRJ1AAAAODjKNwA/ILNZujFG9qqQVSIDqZma+xHnEQNAAAAvo3CDcBvhAU69eZNBSdR+21Xsh77cqNM07Q6FgAAAHBaFG4AfqVR9ImTqH2wdI/Gz1ir3Dy2dAMAAMD3ULgB+J1+LWro2Wtby2EzNHv1AY18Z5lSj7utjgUAAACUQOEG4JeGdozTO6M6KSTAriU7j+q613/VgZTjVscCAAAAilG4AfitHk2qa8Y/uikmzKWthzN0zau/aP0BPjIMAAAAvoHCDcCvtYgN1+wx3dU0JlSJ6Tm6/o0lWrgl0epYAAAAAIUbgP+LrRqkmbd3U/dGkcrMzdfN763QJ8v2Wh0LAAAAlRyFG0CFEBbo1LujOmtw+1rK95i6f9Y6/WfeFj42DAAAAJahcAOoMAIcNj1/XRv9s3djSdIrP27nY8MAAABgGQo3gArFMAyN79tEzw7hY8MAAABgLQo3gAppaKeCjw2r4nJoyc6junbqr9p/LMvqWAAAAKhEKNwAKqweTaprxt+7qUZYoLYlZuia137lY8MAAABQbijcACq05rFhmj32IsXXCNWR9BwNfWOJftzMx4YBAADA+yjcACq8muFBmvmPbrqkcZSycvN183vL9dFve6yOBQAAgAqOwg2gUggNdOqdUZ10XYfa8pjSQ7PX6+m5m+Xx8LFhAAAA8A4KN4BKw2m36dlrW2t83yaSpNcX7dDdM9Yon9INAAAAL6BwA6hUDMPQP3s31vPXtZHDZujzNQf18Jz1Mk1KNwAAAMoWhRtApTSkQ229MrydDEP6eNlevbhgm9WRAAAAUMFQuAFUWv1b1tTjV7eUJL30/TZ9sGS3tYEAAABQoVC4AVRqN3atq7v6NJYkPfLFBn2z7pDFiQAAAFBRULgBVHp39m6sEV3qyDSluz5Zo193JFkdCQAAABUAhRtApWcYhh67uqX6t6ih3HyPbnt/pdYfSLU6FgAAAPwchRsAJNlthqbc0FZd6ldTRk6eRr27XHuPZlkdCwAAAH6Mwg0AhQKddr01sqOa1QxTUkaO/vrObzqSnmN1LAAAAPgpCjcAnCQs0Kn3RndS7Ygg7TmapdHTlik92211LAAAAPghCjcA/EF0WKA+uLmLIkMCtP5Amm54c6kOpR63OhYAAAD8DIUbAE6jflSIpo3urMiQAG04mKarXvlFa/elWB0LAAAAfoTCDQB/olXtcM0Z211NY0J1JD1HQ99Yoq9+P2h1LAAAAPgJCjcAnEFctWB9dns3XRofrZw8j8ZNX60pC7bKNE2rowEAAMDHUbgB4CxCA51666aOuvWS+pKkKQu26Z+frFG2O9/iZAAAAPBlFG4AKAW7zdBDA5rrmSGt5LAZ+nLtQV3/5lIlpmVbHQ0AAAA+isINAOfg+k519MHNXVQ12Km1+1J09au/aMPBVKtjAQAAwAdRuAHgHHVrGKk5Y7qrYfUQHUrN1nWvL9Ev25OsjgUAAAAfQ+EGgPNQLypEs8Z018WNopSVm6/R7y7Xt+sTrI4FAAAAH0LhBoDzFB7k1P9GddTlLWsoN9+jMR+t1Gcr91sdCwAAAD6Cwg0AF8DlsOvlYe00tGNteUzp3plr9c7iXVbHAgAAgA+gcAPABXLYbXpmSGvdcnHBx4Y99tVGvTifz+oGAACo7CjcAFAGDMPQQwOa6Z6+TSRJ//1+myZ9uVEeD6UbAACgsqJwA0AZMQxDd/RurElXtZAkTft1t/712e/Ky/dYnAwAAABWoHADQBkbeVE9PX9dG9lthv5v1X6N+WiVst35VscCAABAOaNwA4AXDOlQW1NHtFeA3abvNh7WTf9bpuTMXKtjAQAAoBxRuAHASy5rUUPTRndSqMuhZbuTdc1rv2h7YobVsQAAAFBOKNwA4EUXNYrS/425SLUjgrTnaJYGv/aLftmeZHUsAAAAlAMKNwB4WZOYUM0Z210d6kYoLTtPI99Zpo+X7bU6FgAAALyMwg0A5SCqiksf3dJFV7eNVZ7H1AOz1umJrzYqn48NAwAAqLAo3ABQTgKddk25vq3u7lPwWd1vL96lv3+wUpk5eRYnAwAAgDdQuAGgHBmGoTv7NNZLw9opwGHTgk2Hde3rS3Qw5bjV0QAAAFDGKNwAYIGr2sTqk9u6KqpKgDYdStPVr/6ilXuSrY4FAACAMkThBgCLtK8ToTljuyu+RqiOpOfo+jeW6r1fd8s0Oa4bAACgIqBwA4CFakcE6/9uv0gDWtdUnsfUo19s0PgZa3U8N9/qaAAAALhAFG4AsFiIy6FXhrXTwwOayW4zNHv1AQ2e+qv2Hs2yOhoAAAAuAIUbAHyAYRi65ZIG+vDmLsXHdV/58s/6cXOi1dEAAABwniwt3D/99JMGDhyo2NhYGYahOXPmWBkHACzXrWGkvrzjYrWNq6q07Dz97b3lmrJgqzx8XjcAAIDfsbRwZ2Zmqk2bNnr11VetjAEAPqVmeJA+/XtXjehSR6YpTVmwTbe8v0KpWW6rowEAAOAcOKx88ssvv1yXX355qdfPyclRTk5O8fW0tDRJktvtltvtu/8QLcrmyxkBiVn1JTZJE6+MV6vYUD3y5Sb9sDlRV778s14d1lbNaoZaHc9yzCr8BbMKf8Gswh/4ypyey/Mbpo98/oxhGJo9e7YGDRr0p+tMnDhRkyZNOmX59OnTFRwc7MV0AGCdfRnSO1vtSs4x5DRMDW3gUedon/jTDQAAUOlkZWVp+PDhSk1NVVhY2BnX9avCfbot3HFxcUpKSjrrC7WS2+3W/Pnz1bdvXzmdTqvjAH+KWfVdKVlu3fPZ7/pp21FJ0g2dauvhK+LlclTOc18yq/AXzCr8BbMKf+Arc5qWlqaoqKhSFW5Ldyk/Vy6XSy6X65TlTqfTL/4w+EtOgFn1PdXDnZo2uote+mGb/vv9Nn2yfL82HUrXazd2UK2qQVbHswyzCn/BrMJfMKvwB1bP6bk8d+XcNAIAfshmM3RXnyZ6d1QnhQc5tXZ/qq586Wf9tPWI1dEAAABwGhRuAPAzvZpG66s7LlarWuE6luXWyHeX6eXvt/HRYQAAAD7G0sKdkZGhNWvWaM2aNZKkXbt2ac2aNdq7d6+VsQDA58VVC9bMf3TTDZ3iZJrS8/O36pb3V+hoRs7Z7wwAAIByYWnhXrFihdq1a6d27dpJksaPH6927drpkUcesTIWAPiFQKddTw9prWeHtFaAw6YfNieq35SfNH/jYaujAQAAQBafNK1Xr17ykZOkA4DfGtopTi1qhenuT9do6+EM3fr+Cl3XobYeGdhcoYGc+AYAAMAqHMMNABVAi9hwfTHuYv29RwMZhjRz5X71n/Kzft2RZHU0AACASovCDQAVRKDTrgeuaKZPb+umuGpBOpByXMPf+k2PfblR2e58q+MBAABUOhRuAKhgOtevprl39tCwznUkSe/8sksDXvpZa/elWBsMAACgkqFwA0AFVMXl0OTBrfTuqE6KDnVpx5FMDZ76q174boty8zxWxwMAAKgUKNwAUIH9JT5a8+7qoStb11S+x9RLP2zX1a/+oo0H06yOBgAAUOFRuAGggosICdArw9vrleHtFBHs1KZDabrqlcV66fttcueztRsAAMBbKNwAUElc2TpW393dU5c1j1Gex9QL87dq8Gu/aktCutXRAAAAKiQKNwBUItVDXXrjrx303xvaKjzIqXUHUjXw5cV6beF25bG1GwAAoExRuAGgkjEMQ1e3raX5d/dQ7/ho5eZ79Oy3WzTk9SXansjWbgAAgLJC4QaASio6LFBvj+yo/1zXRqGBDq3dl6IrXlqsyXM3KTXLbXU8AAAAv0fhBoBKzDAMXduhtr67u4d6Nqmu3DyP3li0Uz2e+1Fv/bRT2e58qyMCAAD4LQo3AEA1w4M0bXQnvTOqo5rEVFHqcbee/GaTej+/SP+3cr/yPabVEQEAAPwOhRsAIKlga/el8TGae2cPPXtta9UIC9SBlOO6Z+ZaDXjpZy3ckijTpHgDAACUFoUbAFCC3WZoaMc4LfxXL93XP16hgQ5tTkjXqHeXa8Tbv2n9gVSrIwIAAPgFCjcA4LQCnXbd3quhfvrXX3TLxfUVYLfp1x1HddUrBSdW4/huAACAM6NwAwDOKCIkQA9f2Vzf39NTA1rXlMeU3li0U5f/92f9tvOo1fEAAAB8FoUbAFAqcdWC9erw9nrrpo6KCXNpV1Kmrn9zqSbMWa+MnDyr4wEAAPgcCjcA4Jz0bR6j7+7uqWGd4yRJHyzdo8teWKSFWxItTgYAAOBbKNwAgHMWHuTU5MGtNf2WLoqrFqSDqdka9e5yjZ+xRscyc62OBwAA4BMo3ACA83ZRoyjNu6uHbr64vgxDmrXqgPq+uEj/W7xLWbnsZg4AACo3CjcA4IIEBzg04crm+r/bL1Lj6CpKysjV419tVPenf9B/F2xTShZbvAEAQOVE4QYAlIn2dSL01T8v1lPXtFLdyGAdy3LrxQVbddHTP+iJrzYqITXb6ogAAADlisINACgzLoddw7vU0Q/39NLLw9qpec0wZeXm6+3Fu3TJsz/ovs9+184jGVbHBAAAKBcOqwMAACoeu83QwDaxurJ1TS3aekSvLdyhZbuS9emKfZqxcp96x8fo2g619Jf4aLkcdqvjAgAAeAWFGwDgNYZhqFfTaPVqGq2Ve5I1deEOLdiUqAWbDmvBpsMKD3LqytY1Nbh9bbWvU1WGYVgdGQAAoMxQuAEA5aJD3Wp6e2Q1bU9M18yV+zVn9QEdTsvRR7/t1Ue/7VW9yGANaldLg9vVVp3IYKvjAgAAXDAKNwCgXDWKDtUDlzfTv/vFa8mOo5q1ar++3ZCg3UezNGXBNk1ZsE0d60ZoaKc4Xdm6poID+F8VAADwT/wrBgBgCbvN0MWNo3Rx4yg9npOneRsSNHv1AS3enqQVe45pxZ5jevzLjbq6Xaxu6FRHLWuFWx0ZAADgnFC4AQCWC3E5NLh9bQ1uX1sJqdmatXq/Pl2+T3uOZunDpXv14dK9al07XDd0qqOr2sbKxWdsAAAAP0DhBgD4lBrhgRrTq5H+0aOhluw8qo+X7dW8DQn6fX+qft+/Tk98vVFXtqqhWjmSaZpWxwUAAPhTFG4AgE+y2Qx1bxSl7o2idDQjR7NWHdDHy/dq55FMzVh5QJJDXxz+Vdd1jNM17WopJizQ6sgAAAAlULgBAD4vsopLt/ZooFsuqa9lu5I1/bc9+mbdQe04kqmn527Ws99uVo8m1TWkfW31bR6jQCef7Q0AAKxH4QYA+A3DMNSlQaTax4XpooB9yq/VWrPXHNLy3ce0cMsRLdxyRGGBDg1sE6shHWqrbe2qstn4bG8AAGANCjcAwC8FOqQrOtTW8K71tSspU7NW7df/rdyvg6nZxZ/tXcXlUPPYMLWuFa5WtcPVqla46kWGUMIBAEC5oHADAPxe/agQ3XNZU93dp4mW7Dyqz1bu17wNCcrIydOyXclatiu5eN1Ql0MtaoWpVa1wtY2LUJcG1RRVxWVhegAAUFFRuAEAFcbJJ1rLy/do+5EMrdufqvUHUvX7gVRtPJim9Jw8Ld2ZrKU7kyXtkiQ1jQlVt4aR6tYwUl3rRyo82GntCwEAABUChRsAUCE57DbF1whTfI0wXdcxTpKUl+/RtsQMrTuQqnX7U7V8d7I2J6Rry+GCr2m/7pZhSC1jw0sU8KAATsIGAADOHYUbAFBpOOw2NasZpmY1wzS0sIQfzcjRb7uS9euOJC3ZcVQ7jmQWFPIDqXrzp51yOWy6qGGkLm0Wo97x0YqtGmTxqwAAAP6Cwg0AqNQiq7h0RauauqJVTUnS4bRsLdlxVEt2HNXi7Uk6kHJcP245oh+3HNEESc1qhql3fLR6N4tWG86CDgAAzoDCDQDASWLCAjWoXS0NaldLpmlqW2KGFmw6rB82JWrV3mPadChNmw6l6ZUftyuqSoC6N4pSk5hQNaxeRY2iq6huZLCcdpvVLwMAAPgACjcAAH/CMAw1iQlVk5hQjenVSMmZuVq0NVELNiXqpy1HlJSRq8/XHCxxH4fNUN3IYDWKLijgDatXUcta4WpUvQpbwwEAqGQo3AAAlFK1kABd0662rmlXW+58j5bvStaqvce040imtidmaMeRDGXl5mvHkUztOJKpeRsOF9831OVQ67hwtYuLUNu4qmpbpyofRwYAQAVH4QYA4Dw47TZd1ChKFzWKKl7m8Zg6lJatHYkZ2p6Yoe1HMrTtcLrWHyj4OLJfth/VL9uPFq8fVy1I7eIi1CauqprVCFV8zTBVCwmw4uUAAAAvoHADAFBGbDZDtaoGqVbVIPVoUr14eV6+R1sPZ2j1vmNaszdFq/elaHtihvYlH9e+5OP6Yu2J3dKjQ11qWiNUzWqGqWlMqOJrhqpRdBW5HHw0GQAA/obCDQCAlznsNjWPDVPz2DCN6FJXkpR63K3f96dozd4U/X4gVVsS0rU3OUuJ6TlKTM/Rz9uSiu9vtxmqFxmsJjGhahxdRY0LjyuvFxVMEQcAwIdRuAEAsEB4kFOXNK6uSxqf2BKekZOnrYfTtflQurYkpGlTQro2H0pTWnZe8XHhc096jKIi3ji6YCt47Ygg1Yoo2MIeWzVIgU7KOAAAVqJwAwDgI6q4HGpfJ0Lt60QULzNNUwlp2dp6uOB48G2HM7QtseB7es6JIq4Npz5eVBWXakUEqXbVgiJep1qwGkSFqF5UiGqEBXLWdAAAvIzCDQCADzMMQzXDg1QzPEg9Tzou3DRNHU7L0dbD6dqWmKGdRzJ0IOW4Dhw7rgMpx5WVm6+kjBwlZeRo7b6UUx430GlTvcgQNageovpRIaofVUX1o4IVVy1Y1au4ZBiUcQAALhSFGwAAP2QYhmqEB6pGeGCJE7RJBWU8JcutAynHtb+wgO8/lqW9R7O0KylTe5OzlO32aHNCujYnpJ/y2EFOu2oXbhGPK/qKCFKdyGDFVg1SqMtBIQcAoBQo3AAAVDCGYSgiJEARIQFqWSv8lNvd+R7tP3Zcu5IytCspq/B7pnYdydShtGwdd+drW2KGtiVmnPbxg5x2xYS5FB0aqOgwl2LCAk9cD3UpPNipsECnwoKcCnU52HUdAFBpUbgBAKhknHZb4W7kIafclpvn0cGU49qbnKV9x7K0NzlL+5MLru9NzlLqcbeOu/O1+2iWdh/NOutzGUbBselFBTws0KHQQIdCXA4FBzgUEmBXsMuhKi57wXWXXSEBjoI3DIIDFBkSoPAgJ6UdAOCXKNwAAKBYgMOmeoUnVjud47n5SkzP1uG0HB1Oy9bhtGwdSS+6nKPE9GylZecp7bhbOXkemaaUnp2n9Ow8HUg5fl6ZbIZUNThA1UICVK3we0RIQRmvFhKgyCqFt4UEKDLEpWohAQpw2C7kxwAAQJmgcAMAgFILCrCrbmSI6kaevpCfLCcvX+mF5buohKcedyszJ0+ZufmF3/OUlXPS5dyC+6Rk5epoZq7Ss/PkMaXkzFwlZ+aWOmeoy1GwS3vhFvXQwNNcdjkU6CzYsh4cYC+8bC9xOdBpl8thk8NOgQcAnDsKNwAA8AqXwy5XFbuiqrjO+zHc+R4dy8xVclaukjMKv2fm6mhGbnEJL/o6mpmrY1m5yveYSs/JU3pOXpm9FrvNkMthK/yyy+U8cTnIaVdQwInvgc6Cy8EBdgXYpZ0HDR37ba+CXM6C+zpsCii8b0DhY4a4ThT/4AAHW+gBoIKgcAMAAJ/ltNsUHRao6LDAUq3v8ZhKy3braGau0o67lZ6dp4ycPKVnF1xOyz5xOSM7T8fd+Tqem6/j7nxl5eaddDlfOXme4sfN95jKyi1YLrnP8VXYNWfP5nO6h8NmKDjArhCXo6DEF5b8ALtNLqe98LtNrsLvAfaCrfAOuyGnzSa7zZDTbhQssxly2gtKfpCz8A2BwjcIAp0nlgUWPq7DbhQ/DsfOA8CFoXADAIAKw2YzVDU4QFWDAy74sfI9pnLzPMrJKyjfOe6TLuflK8ftUXZevrLdHmUVFvXswu9FRT4zx62de/YpKrqG8jwqvm/B43qUm+dRtjtfWe58ZeXkKze/oOTnecyC3fCzy24r/fmwGZLDbpPTdqK822yGbIZkM4yCL1vBZbthyDAK3iRxOWzFJT/gpMuuwkJvtxXct+jx7EbhssLLkmTKlGlKHrPgskzJYxYsM/6Yq/ANAkfhmwwBdqM4n91WkKvoss0oOJO/3TBkP+kNCoe9II/jpDcsbLbCjEbB7XbjpGWFWW2Fr1tS8fMYKniOoucCUHlRuAEAAE7DbjMKtgQH2M/7Mdxut775Zo+uuKKtnE7n2dfP9xRuSS84nj0rJ1+ZuXnKdheU9Nz8guJf8D2/xHV3vqm8fI/yPKbc+R7le8yCZR6P8vJN5RSW++Pu/BPfT3qDINvtOSWPxyw4c33B0fP55/1zqOzsthNvSBRdttlOvPFgGCos6QXrGzp5mXHiMQrfMCi6n+2kNypM01S+p+DLNKV805THYxZ8N00ZKngTIcBhV4D9xF4PTnvBl8MmHU6w6bv032Wz24rzFL1dYBS+kaCzvH9gqOiNBp30ZsSJZYYMeUxTHlOSTHk8Kr5e9CaLYRTs5VH0Jo+98E0Ou+3Emz4n8pX8+ZX2DQ6z8M0bUyr8fuK6TLPge/FrKnjwP/6ObEbBm3xFOWwnvWabceL2ot/Tid+7SizzmEU/g8IvT8Hvzyz6uZiS3Vb0Js6J+xe9cVSaHVE8ppTnKfi7kJdvKs9jKt/jKfxecN009YffXcFrLnrjyFb4A7AZJX/PJ9YzCn+eZvHPtCi/p+hnWvgLPvn3d/JsyJACnXZd1Sa2VL9Hf+AThfvVV1/Vc889p4SEBLVp00Yvv/yyOnfubHUsAACAcuW02xQeZFN40NnLeVkzzYJ/dOflm3IXlvS8fI/cnsLv+QX/MPeYfyh1hcXOY6qw5HvkzvcUv0FQ9N2dd+KNgaIiWFQQi8thYfkqUnKL8Yl/2JtSYWnwFD5fyax5+YU5TRWWlqLsOlFMTRUUjuLyceobFfmeglx5ha+7qJScq3yPqXydxx3LnU2rjyZYHQKVXESwk8Jdlj799FONHz9er7/+urp06aIpU6aoX79+2rJli6Kjo62OBwAAUCkYRsEWUKddCtL5b9Wv6IoKe1EJL9oqWnRZ5sm7w5vFWy+L3qwo2nr5xzcvirapFhX6P251PXn94jc4it/sMIu3rJ6y9btwK6tpFryRkJvnOfGmyEnXs3PdWr9+g5o1by6bzV68pfLE6z6R8eRlJa7/Mfdpfj6mVLy1/8SW05OuF24BP3nLa/GW2JPeHDn5Z2MWbi03zRPPb5xtU7yK3sSR9Iet5EVv8pz8mgqf4aTLJ17fyb9n8w/fi95UKr5sqvDNJbN4K3bR1uMTh0AU7slQuJW5aO5Ofp6iWSq6fNbXa0hO+4m9BByFh1EU7zVQ+Dso/r3ppD0P/vC6Tt5y/cd1T9kifpo9Hko+VsnHMU1TIS7LK2qZsvzVvPDCC7r11ls1evRoSdLrr7+ur7/+Wu+8847uv/9+i9MBAAAAJxiFx3M7Kth7Em63W98cXa8rutUt1eEPAErH0sKdm5urlStX6oEHHiheZrPZ1KdPHy1ZsuSU9XNycpSTk1N8PS0tTVLBHwi3+1zPGFp+irL5ckZAYlbhP5hV+AtmFf6CWYU/8JU5PZfnt7RwJyUlKT8/XzExMSWWx8TEaPPmUz8+Y/LkyZo0adIpy7/77jsFBwd7LWdZmT9/vtURgFJhVuEvmFX4C2YV/oJZhT+wek6zsrJKva7lu5SfiwceeEDjx48vvp6Wlqa4uDhddtllCgsLszDZmbndbs2fP199+/ZlFx34NGYV/oJZhb9gVuEvmFX4A1+Z06I9rUvD0sIdFRUlu92uw4cPl1h++PBh1ahR45T1XS6XXC7XKcudTqdf/GHwl5wAswp/wazCXzCr8BfMKvyB1XN6Ls9t82KOswoICFCHDh30/fffFy/zeDz6/vvv1a1bNwuTAQAAAABwYSzfpXz8+PEaOXKkOnbsqM6dO2vKlCnKzMwsPms5AAAAAAD+yPLCff311+vIkSN65JFHlJCQoLZt2+rbb7895URqAAAAAAD4E8sLtySNGzdO48aNszoGAAAAAABlxtJjuAEAAAAAqKgo3AAAAAAAeAGFGwAAAAAAL6BwAwAAAADgBRRuAAAAAAC8gMINAAAAAIAXULgBAAAAAPACCjcAAAAAAF5A4QYAAAAAwAso3AAAAAAAeIHD6gAXwjRNSVJaWprFSc7M7XYrKytLaWlpcjqdVscB/hSzCn/BrMJfMKvwF8wq/IGvzGlR/yzqo2fi14U7PT1dkhQXF2dxEgAAAABAZZKenq7w8PAzrmOYpanlPsrj8ejgwYMKDQ2VYRhWx/lTaWlpiouL0759+xQWFmZ1HOBPMavwF8wq/AWzCn/BrMIf+Mqcmqap9PR0xcbGymY781Hafr2F22azqXbt2lbHKLWwsDD+gMEvMKvwF8wq/AWzCn/BrMIf+MKcnm3LdhFOmgYAAAAAgBdQuAEAAAAA8AIKdzlwuVx69NFH5XK5rI4CnBGzCn/BrMJfMKvwF8wq/IE/zqlfnzQNAAAAAABfxRZuAAAAAAC8gMINAAAAAIAXULgBAAAAAPACCjcAAAAAAF5A4S4Hr776qurVq6fAwEB16dJFy5YtszoSKrHJkyerU6dOCg0NVXR0tAYNGqQtW7aUWCc7O1tjx45VZGSkqlSpoiFDhujw4cMWJQYKPP300zIMQ3fddVfxMmYVvuLAgQO68cYbFRkZqaCgILVq1UorVqwovt00TT3yyCOqWbOmgoKC1KdPH23bts3CxKiM8vPzNWHCBNWvX19BQUFq2LChHn/8cZ18DmVmFVb46aefNHDgQMXGxsowDM2ZM6fE7aWZy+TkZI0YMUJhYWGqWrWqbr75ZmVkZJTjqzg9CreXffrppxo/frweffRRrVq1Sm3atFG/fv2UmJhodTRUUosWLdLYsWO1dOlSzZ8/X263W5dddpkyMzOL17n77rv15ZdfaubMmVq0aJEOHjyowYMHW5gald3y5cv1xhtvqHXr1iWWM6vwBceOHVP37t3ldDo1d+5cbdy4Uc8//7wiIiKK13n22Wf10ksv6fXXX9dvv/2mkJAQ9evXT9nZ2RYmR2XzzDPPaOrUqXrllVe0adMmPfPMM3r22Wf18ssvF6/DrMIKmZmZatOmjV599dXT3l6auRwxYoQ2bNig+fPn66uvvtJPP/2k2267rbxewp8z4VWdO3c2x44dW3w9Pz/fjI2NNSdPnmxhKuCExMREU5K5aNEi0zRNMyUlxXQ6nebMmTOL19m0aZMpyVyyZIlVMVGJpaenm40bNzbnz59v9uzZ07zzzjtN02RW4Tvuu+8+8+KLL/7T2z0ej1mjRg3zueeeK16WkpJiulwu8+OPPy6PiIBpmqY5YMAA829/+1uJZYMHDzZHjBhhmiazCt8gyZw9e3bx9dLM5caNG01J5vLly4vXmTt3rmkYhnngwIFyy346bOH2otzcXK1cuVJ9+vQpXmaz2dSnTx8tWbLEwmTACampqZKkatWqSZJWrlwpt9tdYm7j4+NVp04d5haWGDt2rAYMGFBiJiVmFb7jiy++UMeOHXXdddcpOjpa7dq101tvvVV8+65du5SQkFBiVsPDw9WlSxdmFeXqoosu0vfff6+tW7dKktauXavFixfr8ssvl8SswjeVZi6XLFmiqlWrqmPHjsXr9OnTRzabTb/99lu5Zz6Zw9Jnr+CSkpKUn5+vmJiYEstjYmK0efNmi1IBJ3g8Ht11113q3r27WrZsKUlKSEhQQECAqlatWmLdmJgYJSQkWJASldknn3yiVatWafny5afcxqzCV+zcuVNTp07V+PHj9eCDD2r58uX65z//qYCAAI0cObJ4Hk/37wFmFeXp/vvvV1pamuLj42W325Wfn68nn3xSI0aMkCRmFT6pNHOZkJCg6OjoErc7HA5Vq1bN8tmlcAOV2NixY7V+/XotXrzY6ijAKfbt26c777xT8+fPV2BgoNVxgD/l8XjUsWNHPfXUU5Kkdu3aaf369Xr99dc1cuRIi9MBJ8yYMUMfffSRpk+frhYtWmjNmjW66667FBsby6wCXsIu5V4UFRUlu91+yhlzDx8+rBo1aliUCigwbtw4ffXVV/rxxx9Vu3bt4uU1atRQbm6uUlJSSqzP3KK8rVy5UomJiWrfvr0cDoccDocWLVqkl156SQ6HQzExMcwqfELNmjXVvHnzEsuaNWumvXv3SlLxPPLvAVjtX//6l+6//37dcMMNatWqlf7617/q7rvv1uTJkyUxq/BNpZnLGjVqnHJS6ry8PCUnJ1s+uxRuLwoICFCHDh30/fffFy/zeDz6/vvv1a1bNwuToTIzTVPjxo3T7Nmz9cMPP6h+/folbu/QoYOcTmeJud2yZYv27t3L3KJc9e7dW+vWrdOaNWuKvzp27KgRI0YUX2ZW4Qu6d+9+yscrbt26VXXr1pUk1a9fXzVq1Cgxq2lpafrtt9+YVZSrrKws2Wwl//lvt9vl8XgkMavwTaWZy27duiklJUUrV64sXueHH36Qx+NRly5dyj3zydil3MvGjx+vkSNHqmPHjurcubOmTJmizMxMjR492upoqKTGjh2r6dOn6/PPP1doaGjxcS3h4eEKCgpSeHi4br75Zo0fP17VqlVTWFiY7rjjDnXr1k1du3a1OD0qk9DQ0OJzCxQJCQlRZGRk8XJmFb7g7rvv1kUXXaSnnnpKQ4cO1bJly/Tmm2/qzTfflKTiz49/4okn1LhxY9WvX18TJkxQbGysBg0aZG14VCoDBw7Uk08+qTp16qhFixZavXq1XnjhBf3tb3+TxKzCOhkZGdq+fXvx9V27dmnNmjWqVq2a6tSpc9a5bNasmfr3769bb71Vr7/+utxut8aNG6cbbrhBsbGxFr2qQpaeI72SePnll806deqYAQEBZufOnc2lS5daHQmVmKTTfr377rvF6xw/ftwcM2aMGRERYQYHB5vXXHONeejQIetCA4VO/lgw02RW4Tu+/PJLs2XLlqbL5TLj4+PNN998s8TtHo/HnDBhghkTE2O6XC6zd+/e5pYtWyxKi8oqLS3NvPPOO806deqYgYGBZoMGDcyHHnrIzMnJKV6HWYUVfvzxx9P++3TkyJGmaZZuLo8ePWoOGzbMrFKlihkWFmaOHj3aTE9Pt+DVlGSYpmla1PUBAAAAAKiwOIYbAAAAAAAvoHADAAAAAOAFFG4AAAAAALyAwg0AAAAAgBdQuAEAAAAA8AIKNwAAAAAAXkDhBgAAAADACyjcAAAAAAB4AYUbAACUmmEYmjNnjtUxAADwCxRuAAD8xKhRo2QYxilf/fv3tzoaAAA4DYfVAQAAQOn1799f7777bollLpfLojQAAOBM2MINAIAfcblcqlGjRomviIgISQW7e0+dOlWXX365goKC1KBBA3322Wcl7r9u3TpdeumlCgoKUmRkpG677TZlZGSUWOedd95RixYt5HK5VLNmTY0bN67E7UlJSbrmmmsUHBysxo0b64svvvDuiwYAwE9RuAEAqEAmTJigIUOGaO3atRoxYoRuuOEGbdq0SZKUmZmpfv36KSIiQsuXL9fMmTO1YMGCEoV66tSpGjt2rG677TatW7dOX3zxhRo1alTiOSZNmqShQ4fq999/1xVXXKERI0YoOTm5XF8nAAD+wDBN07Q6BAAAOLtRo0bpww8/VGBgYInlDz74oB588EEZhqF//OMfmjp1avFtXbt2Vfv27fXaa6/prbfe0n333ad9+/YpJCREkvTNN99o4MCBOnjwoGJiYlSrVi2NHj1aTzzxxGkzGIahhx9+WI8//rikghJfpUoVzZ07l2PJAQD4A47hBgDAj/zlL38pUaglqVq1asWXu3XrVuK2bt26ac2aNZKkTZs2qU2bNsVlW5K6d+8uj8ejLVu2yDAMHTx4UL179z5jhtatWxdfDgkJUVhYmBITE8/3JQEAUGFRuAEA8CMhISGn7OJdVoKCgkq1ntPpLHHdMAx5PB5vRAIAwK9xDDcAABXI0qVLT7nerFkzSVKzZs20du1aZWZmFt/+yy+/yGazqWnTpgoNDVW9evX0/fffl2tmAAAqKrZwAwDgR3JycpSQkFBimcPhUFRUlCRp5syZ6tixoy6++GJ99NFHWrZsmf73v/9JkkaMGKFHH31UI0eO1MSJE3XkyBHdcccd+utf/6qYmBhJ0sSJE/WPf/xD0dHRuvzyy5Wenq5ffvlFd9xxR/m+UAAAKgAKNwAAfuTbb79VzZo1Syxr2rSpNm/eLKngDOKffPKJxowZo5o1a+rjjz9W8+bNJUnBwcGaN2+e7rzzTnXq1EnBwcEaMmSIXnjhheLHGjlypLKzs/Xiiy/q3nvvVVRUlK699trye4EAAFQgnKUcAIAKwjAMzZ49W4MGDbI6CgAAEMdwAwAAAADgFRRuAAAAAAC8gGO4AQCoIDhKDAAA38IWbgAAAAAAvIDCDQAAAACAF1C4AQAAAADwAgo3AAAAAABeQOEGAAAAAMALKNwAAAAAAHgBhRsAAAAAAC+gcAMAAAAA4AX/Dyv/aJaL/lnDAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- PROFESSIONAL POST ---\n",
            "proud to announce that our team successfully launched the new customer portal ahead of schedule! this project wouldn ' t have been possible without the dedication and collaboration of our amazing development team. #teamwork #projectsuccess #softwaredevelopment\n",
            "--------------------------------------------------\n",
            "\n",
            "--- MOTIVATIONAL POST ---\n",
            "failed my first technical interview five years ago. today i ' m leading a team of 15 engineers. sometimes rejection is just redirection toward something better. keep pushing forward. #resilience #careergrowth #nevergiveup\n",
            "--------------------------------------------------\n",
            "\n",
            "--- INDUSTRY_INSIGHTS POST ---\n",
            "remote work has fundamentally changed how we think about talent acquisition. geography is no longer a barrier to incorporate ai tools will have a significant competitive advantage. the question isn ' t whether to adopt ai, but how quickly you can do it effectively. #ai #innovation #futureofwork\n",
            "--------------------------------------------------\n",
            "\n",
            "--- PERSONAL_STORY POST ---\n",
            "moved to no as city with developer making 35k no job, and just enough savings for three months. two years later, i ' ve built meaningful professional relationships and found my dream role. sometimes sometimes each risks lead to the biggest rewards. #takingrisks #networking #careerchange\n",
            "--------------------------------------------------\n",
            "\n",
            "--- THOUGHT_LEADERSHIP POST ---\n",
            "data without context is just noise one thing in thousands of datasets, i ' ve learned that the most own recognition. when you invest in people, everything else follows. #leadership #teamdevelopment #peoplefirst\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import pickle\n",
        "from typing import List, Dict, Optional, Tuple\n",
        "import math\n",
        "import random\n",
        "from pathlib import Path\n",
        "import logging\n",
        "from dataclasses import dataclass\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    vocab_size: int = 10000\n",
        "    max_seq_length: int = 512\n",
        "    d_model: int = 512\n",
        "    n_heads: int = 8\n",
        "    n_layers: int = 6\n",
        "    d_ff: int = 2048\n",
        "    dropout: float = 0.1\n",
        "    learning_rate: float = 1e-4\n",
        "    batch_size: int = 16\n",
        "    num_epochs: int = 100\n",
        "    warmup_steps: int = 1000\n",
        "    max_grad_norm: float = 1.0\n",
        "    num_themes: int = 20\n",
        "\n",
        "class ImprovedTokenizer:\n",
        "    def __init__(self):\n",
        "        self.vocab = {}\n",
        "        self.reverse_vocab = {}\n",
        "        self.special_tokens = {\n",
        "            '<PAD>': 0,\n",
        "            '<UNK>': 1,\n",
        "            '<BOS>': 2,\n",
        "            '<EOS>': 3,\n",
        "            '<MASK>': 4\n",
        "        }\n",
        "        self.vocab.update(self.special_tokens)\n",
        "        self.reverse_vocab.update({v: k for k, v in self.special_tokens.items()})\n",
        "\n",
        "    def build_vocab(self, texts: List[str], min_freq: int = 1):\n",
        "        word_counts = {}\n",
        "\n",
        "        for text in texts:\n",
        "            tokens = self._tokenize(text)\n",
        "            for token in tokens:\n",
        "                word_counts[token] = word_counts.get(token, 0) + 1\n",
        "\n",
        "        vocab_size = len(self.special_tokens)\n",
        "        for word, count in sorted(word_counts.items(), key=lambda x: x[1], reverse=True):\n",
        "            if count >= min_freq and vocab_size < 15000:\n",
        "                self.vocab[word] = vocab_size\n",
        "                self.reverse_vocab[vocab_size] = word\n",
        "                vocab_size += 1\n",
        "\n",
        "        logger.info(f\"Built vocabulary with {len(self.vocab)} tokens\")\n",
        "\n",
        "    def _tokenize(self, text: str) -> List[str]:\n",
        "        text = re.sub(r'[^\\w\\s#@.,!?\\-\\'\":]', ' ', text.lower())\n",
        "\n",
        "        tokens = []\n",
        "        words = text.split()\n",
        "\n",
        "        for word in words:\n",
        "            if word.startswith('#') or word.startswith('@'):\n",
        "                tokens.append(word)\n",
        "            else:\n",
        "                sub_tokens = re.findall(r'\\w+|[.,!?\\-\\'\":]', word)\n",
        "                tokens.extend(sub_tokens)\n",
        "\n",
        "        return [token for token in tokens if token.strip()]\n",
        "\n",
        "    def encode(self, text: str, max_length: int = 512) -> List[int]:\n",
        "        tokens = self._tokenize(text)\n",
        "        token_ids = [self.special_tokens['<BOS>']]\n",
        "\n",
        "        for token in tokens:\n",
        "            if len(token_ids) >= max_length - 1:\n",
        "                break\n",
        "            token_ids.append(self.vocab.get(token, self.special_tokens['<UNK>']))\n",
        "\n",
        "        token_ids.append(self.special_tokens['<EOS>'])\n",
        "\n",
        "        while len(token_ids) < max_length:\n",
        "            token_ids.append(self.special_tokens['<PAD>'])\n",
        "\n",
        "        return token_ids[:max_length]\n",
        "\n",
        "    def decode(self, token_ids: List[int]) -> str:\n",
        "        tokens = []\n",
        "        for token_id in token_ids:\n",
        "            if token_id == self.special_tokens['<PAD>']:\n",
        "                continue\n",
        "            if token_id == self.special_tokens['<EOS>']:\n",
        "                break\n",
        "            if token_id == self.special_tokens['<BOS>']:\n",
        "                continue\n",
        "            token = self.reverse_vocab.get(token_id, '<UNK>')\n",
        "            if token != '<UNK>':\n",
        "                tokens.append(token)\n",
        "\n",
        "        text = ' '.join(tokens)\n",
        "        text = re.sub(r'\\s+([.,!?])', r'\\1', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        return text.strip()\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_model // n_heads\n",
        "\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.scale = math.sqrt(self.d_k)\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "        seq_len = query.size(1)\n",
        "\n",
        "        Q = self.w_q(query).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.w_k(key).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.w_v(value).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            causal_mask = torch.tril(torch.ones(seq_len, seq_len)).bool().to(query.device)\n",
        "            causal_mask = causal_mask.unsqueeze(0).unsqueeze(1).expand(batch_size, self.n_heads, -1, -1)\n",
        "            scores = scores.masked_fill(~causal_mask, -1e9)\n",
        "\n",
        "        attention = F.softmax(scores, dim=-1)\n",
        "        attention = self.dropout(attention)\n",
        "\n",
        "        output = torch.matmul(attention, V)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
        "\n",
        "        return self.w_o(output)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.dropout(F.gelu(self.linear1(x))))\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, n_heads: int, d_ff: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(d_model, n_heads, dropout)\n",
        "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        norm_x = self.norm1(x)\n",
        "        attn_output = self.attention(norm_x, norm_x, norm_x, mask)\n",
        "        x = x + self.dropout(attn_output)\n",
        "\n",
        "        norm_x = self.norm2(x)\n",
        "        ff_output = self.feed_forward(norm_x)\n",
        "        x = x + self.dropout(ff_output)\n",
        "\n",
        "        return x\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        return x + self.pe[:, :seq_len, :]\n",
        "\n",
        "class LinkedInLLM(nn.Module):\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.token_embedding = nn.Embedding(config.vocab_size, config.d_model)\n",
        "        self.positional_encoding = PositionalEncoding(config.d_model, config.max_seq_length)\n",
        "\n",
        "        self.transformer_blocks = nn.ModuleList([\n",
        "            TransformerBlock(config.d_model, config.n_heads, config.d_ff, config.dropout)\n",
        "            for _ in range(config.n_layers)\n",
        "        ])\n",
        "\n",
        "        self.ln_f = nn.LayerNorm(config.d_model)\n",
        "        self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n",
        "\n",
        "        self.theme_embedding = nn.Embedding(config.num_themes, config.d_model)\n",
        "        self.theme_projection = nn.Linear(config.d_model, config.d_model)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    def forward(self, input_ids, theme_ids=None, attention_mask=None):\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "\n",
        "        token_embeddings = self.token_embedding(input_ids)\n",
        "\n",
        "        if theme_ids is not None:\n",
        "            theme_embeddings = self.theme_embedding(theme_ids)\n",
        "            theme_embeddings = self.theme_projection(theme_embeddings).unsqueeze(1)\n",
        "            token_embeddings = token_embeddings + theme_embeddings\n",
        "\n",
        "        x = self.positional_encoding(token_embeddings)\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        for block in self.transformer_blocks:\n",
        "            x = block(x, attention_mask)\n",
        "\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "class LinkedInDataset(Dataset):\n",
        "    def __init__(self, texts: List[str], themes: List[int], tokenizer: ImprovedTokenizer, max_length: int = 512):\n",
        "        self.texts = texts\n",
        "        self.themes = themes\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "        theme = self.themes[idx]\n",
        "\n",
        "        token_ids = self.tokenizer.encode(text, self.max_length)\n",
        "\n",
        "        input_ids = torch.tensor(token_ids[:-1], dtype=torch.long)\n",
        "        target_ids = torch.tensor(token_ids[1:], dtype=torch.long)\n",
        "        theme_id = torch.tensor(theme, dtype=torch.long)\n",
        "\n",
        "        return input_ids, target_ids, theme_id\n",
        "\n",
        "class LinkedInPostGenerator:\n",
        "    def __init__(self, config: ModelConfig):\n",
        "        self.config = config\n",
        "        self.tokenizer = ImprovedTokenizer()\n",
        "        self.model = None\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "        self.themes = {\n",
        "            'professional': 0,\n",
        "            'motivational': 1,\n",
        "            'industry_insights': 2,\n",
        "            'personal_story': 3,\n",
        "            'thought_leadership': 4,\n",
        "            'company_update': 5,\n",
        "            'networking': 6,\n",
        "            'career_advice': 7,\n",
        "            'innovation': 8,\n",
        "            'general': 9,\n",
        "            'career_growth': 10,\n",
        "            'career_development': 11,\n",
        "            'industry_trends': 12,\n",
        "            'personal_branding': 13,\n",
        "            'community_building': 14,\n",
        "            'educational': 15,\n",
        "            'inspirational': 16,\n",
        "            'workplace_culture': 17,\n",
        "            'achievement': 18,\n",
        "            'startup': 19\n",
        "        }\n",
        "\n",
        "        self.sample_data = self._generate_comprehensive_sample_data()\n",
        "\n",
        "    def _generate_comprehensive_sample_data(self) -> List[Tuple[str, str]]:\n",
        "        samples = [\n",
        "            (\"Just received my AWS Solutions Architect certification after months of preparation! The journey was challenging but incredibly rewarding. Excited to apply these cloud computing skills to help businesses scale efficiently. #AWS #CloudComputing #Certification #ProfessionalGrowth\", \"professional\"),\n",
        "            (\"Proud to announce that our team successfully launched the new customer portal ahead of schedule! This project wouldn't have been possible without the dedication and collaboration of our amazing development team. #TeamWork #ProjectSuccess #SoftwareDevelopment\", \"professional\"),\n",
        "            (\"Every expert was once a beginner. Every professional was once an amateur. The key is to never stop learning and never stop improving. Your journey is unique, embrace it. #Motivation #LearningJourney #GrowthMindset\", \"motivational\"),\n",
        "            (\"Failed my first technical interview five years ago. Today I'm leading a team of 15 engineers. Sometimes rejection is just redirection toward something better. Keep pushing forward. #Resilience #CareerGrowth #NeverGiveUp\", \"motivational\"),\n",
        "            (\"The AI revolution isn't coming - it's here. Companies that adapt their workflows to incorporate AI tools will have a significant competitive advantage. The question isn't whether to adopt AI, but how quickly you can do it effectively. #AI #Innovation #FutureOfWork\", \"industry_insights\"),\n",
        "            (\"Remote work has fundamentally changed how we think about talent acquisition. Geography is no longer a barrier to finding the best people. Companies that embrace this shift will access a global talent pool. #RemoteWork #TalentAcquisition #GlobalWorkforce\", \"industry_insights\"),\n",
        "            (\"Started my career as a junior developer making $35K. Today I'm a CTO. The path wasn't linear - I changed companies four times, learned five programming languages, and made countless mistakes. Each step taught me something valuable. #CareerJourney #Leadership #TechCareer\", \"personal_story\"),\n",
        "            (\"Moved to a new city with no network, no job, and just enough savings for three months. Two years later, I've built meaningful professional relationships and found my dream role. Sometimes the biggest risks lead to the biggest rewards. #TakingRisks #Networking #CareerChange\", \"personal_story\"),\n",
        "            (\"Data without context is just noise. After analyzing thousands of datasets, I've learned that the most valuable insights come from understanding the story behind the numbers. Always ask why before diving into the what. #DataScience #Analytics #BusinessIntelligence\", \"thought_leadership\"),\n",
        "            (\"The best leaders I know have one thing in common: they prioritize their team's growth over their own recognition. When you invest in people, everything else follows. #Leadership #TeamDevelopment #PeopleFirst\", \"thought_leadership\"),\n",
        "            (\"Excited to share that our company has been recognized as one of the top 50 startups to watch this year! This achievement reflects the hard work of our entire team and the trust our customers place in us. #CompanyNews #StartupLife #Recognition\", \"company_update\"),\n",
        "            (\"We're hiring! Looking for passionate software engineers to join our growing team. If you're excited about building products that make a difference, I'd love to connect. #Hiring #SoftwareEngineer #JoinOurTeam\", \"company_update\"),\n",
        "            (\"Had an incredible conversation with a fellow entrepreneur today about the challenges of scaling a tech startup. Sometimes the best insights come from peer-to-peer learning. Always open to connecting with like-minded professionals. #Networking #Entrepreneurship #Learning\", \"networking\"),\n",
        "            (\"Attended an amazing tech conference this week. The speaker lineup was incredible, but the real value was in the conversations between sessions. Nothing beats learning from others' experiences. #TechConference #Learning #Community\", \"networking\"),\n",
        "            (\"Three things I wish I knew when starting my career: 1) Your network is your net worth - invest in relationships 2) Don't be afraid to ask questions - curiosity is a superpower 3) Document your wins - your memory isn't as good as you think. #CareerAdvice #ProfessionalDevelopment\", \"career_advice\"),\n",
        "            (\"Negotiating your salary isn't just about the money - it's about recognizing your value. Research market rates, document your achievements, and remember that the worst they can say is no. You miss 100% of the shots you don't take. #SalaryNegotiation #KnowYourWorth\", \"career_advice\"),\n",
        "            (\"The most innovative solutions often come from combining existing technologies in new ways. We don't always need to reinvent the wheel - sometimes we just need to put it in a different context. #Innovation #Technology #ProblemSolving\", \"innovation\"),\n",
        "            (\"Automation isn't about replacing humans - it's about freeing them to do more creative and strategic work. The future belongs to those who can work alongside technology, not compete with it. #Automation #FutureOfWork #HumanTechCollaboration\", \"innovation\"),\n",
        "            (\"Five principles for clean code: 1) Write code for humans, not computers 2) Use meaningful names 3) Keep functions small and focused 4) Comment the why, not the what 5) Test everything. Your future self will thank you. #CleanCode #SoftwareDevelopment #BestPractices\", \"educational\"),\n",
        "            (\"The STAR method for behavioral interviews: Situation, Task, Action, Result. This framework helps you tell compelling stories about your experience. Practice it, and you'll never struggle with behavioral questions again. #InterviewTips #CareerAdvice #ProfessionalDevelopment\", \"educational\"),\n",
        "            (\"Your comfort zone is a beautiful place, but nothing ever grows there. Every major breakthrough in my career happened when I took on something that scared me. Embrace the discomfort - that's where growth happens. #GrowthMindset #ComfortZone #PersonalDevelopment\", \"inspirational\"),\n",
        "            (\"The difference between successful people and others isn't talent or luck - it's consistency. Small daily actions compound into extraordinary results over time. What small action will you take today? #Consistency #Success #DailyHabits\", \"inspirational\"),\n",
        "        ]\n",
        "\n",
        "        expanded_samples = []\n",
        "        for post, theme in samples:\n",
        "            expanded_samples.append((post, theme))\n",
        "            for i in range(3):\n",
        "                expanded_samples.append((post, theme))\n",
        "\n",
        "        return expanded_samples\n",
        "\n",
        "    def prepare_data(self):\n",
        "        texts = [sample[0] for sample in self.sample_data]\n",
        "        themes = [self.themes[sample[1]] for sample in self.sample_data]\n",
        "\n",
        "        self.tokenizer.build_vocab(texts)\n",
        "        self.config.vocab_size = len(self.tokenizer.vocab)\n",
        "\n",
        "        dataset = LinkedInDataset(texts, themes, self.tokenizer, self.config.max_seq_length)\n",
        "\n",
        "        dataloader = DataLoader(\n",
        "            dataset,\n",
        "            batch_size=self.config.batch_size,\n",
        "            shuffle=True,\n",
        "            num_workers=0,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        return dataloader\n",
        "\n",
        "    def train(self, dataloader):\n",
        "        self.config.vocab_size = len(self.tokenizer.vocab)\n",
        "        self.model = LinkedInLLM(self.config).to(self.device)\n",
        "\n",
        "        optimizer = optim.AdamW(\n",
        "            self.model.parameters(),\n",
        "            lr=self.config.learning_rate,\n",
        "            weight_decay=0.01,\n",
        "            betas=(0.9, 0.999)\n",
        "        )\n",
        "\n",
        "        scheduler = optim.lr_scheduler.OneCycleLR(\n",
        "            optimizer,\n",
        "            max_lr=self.config.learning_rate,\n",
        "            steps_per_epoch=len(dataloader),\n",
        "            epochs=self.config.num_epochs,\n",
        "            pct_start=0.1\n",
        "        )\n",
        "\n",
        "        self.model.train()\n",
        "        losses = []\n",
        "        best_loss = float('inf')\n",
        "\n",
        "        for epoch in range(self.config.num_epochs):\n",
        "            total_loss = 0\n",
        "            progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{self.config.num_epochs}')\n",
        "\n",
        "            for batch_idx, (input_ids, target_ids, theme_ids) in enumerate(progress_bar):\n",
        "                input_ids = input_ids.to(self.device, non_blocking=True)\n",
        "                target_ids = target_ids.to(self.device, non_blocking=True)\n",
        "                theme_ids = theme_ids.to(self.device, non_blocking=True)\n",
        "\n",
        "                logits = self.model(input_ids, theme_ids)\n",
        "\n",
        "                loss = F.cross_entropy(\n",
        "                    logits.view(-1, logits.size(-1)),\n",
        "                    target_ids.view(-1),\n",
        "                    ignore_index=self.tokenizer.special_tokens['<PAD>']\n",
        "                )\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.config.max_grad_norm)\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "                progress_bar.set_postfix({\n",
        "                    'loss': loss.item(),\n",
        "                    'lr': scheduler.get_last_lr()[0]\n",
        "                })\n",
        "\n",
        "            avg_loss = total_loss / len(dataloader)\n",
        "            losses.append(avg_loss)\n",
        "\n",
        "            logger.info(f'Epoch {epoch+1}/{self.config.num_epochs}, Average Loss: {avg_loss:.4f}')\n",
        "\n",
        "            if avg_loss < best_loss:\n",
        "                best_loss = avg_loss\n",
        "                self.save_model('best_linkedin_llm.pth')\n",
        "\n",
        "            if (epoch + 1) % 20 == 0:\n",
        "                self.save_model(f'checkpoint_epoch_{epoch+1}.pth')\n",
        "\n",
        "        return losses\n",
        "\n",
        "    def generate_post(self, theme: str, max_length: int = 150, temperature: float = 0.7, top_p: float = 0.9) -> str:\n",
        "        if self.model is None:\n",
        "            raise ValueError(\"Model not trained yet. Call train() first.\")\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            input_ids = [self.tokenizer.special_tokens['<BOS>']]\n",
        "            theme_id = self.themes.get(theme, self.themes['general'])\n",
        "\n",
        "            for _ in range(max_length):\n",
        "                input_tensor = torch.tensor([input_ids], dtype=torch.long).to(self.device)\n",
        "                theme_tensor = torch.tensor([theme_id], dtype=torch.long).to(self.device)\n",
        "\n",
        "                logits = self.model(input_tensor, theme_tensor)\n",
        "\n",
        "                logits = logits[0, -1, :] / temperature\n",
        "\n",
        "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "                sorted_indices_to_remove = cumulative_probs > top_p\n",
        "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "                sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
        "                logits[indices_to_remove] = -float('inf')\n",
        "\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "\n",
        "                probs[self.tokenizer.special_tokens['<PAD>']] = 0\n",
        "                probs = probs / probs.sum()\n",
        "\n",
        "                next_token = torch.multinomial(probs, 1).item()\n",
        "\n",
        "                if next_token == self.tokenizer.special_tokens['<EOS>']:\n",
        "                    break\n",
        "\n",
        "                input_ids.append(next_token)\n",
        "\n",
        "            generated_text = self.tokenizer.decode(input_ids)\n",
        "            return generated_text\n",
        "\n",
        "    def save_model(self, filepath: str):\n",
        "        torch.save({\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'config': self.config,\n",
        "            'tokenizer': self.tokenizer,\n",
        "            'themes': self.themes\n",
        "        }, filepath)\n",
        "        logger.info(f\"Model saved to {filepath}\")\n",
        "\n",
        "    def load_model(self, filepath: str):\n",
        "        torch.serialization.add_safe_globals([ModelConfig, ImprovedTokenizer])\n",
        "        checkpoint = torch.load(filepath, map_location=self.device)\n",
        "\n",
        "        self.config = checkpoint['config']\n",
        "        self.tokenizer = checkpoint['tokenizer']\n",
        "        self.themes = checkpoint['themes']\n",
        "\n",
        "        self.model = LinkedInLLM(self.config).to(self.device)\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.model.eval()\n",
        "\n",
        "        logger.info(f\"Model loaded from {filepath}\")\n",
        "\n",
        "def main():\n",
        "    config = ModelConfig(\n",
        "        vocab_size=15000,\n",
        "        max_seq_length=256,\n",
        "        d_model=384,\n",
        "        n_heads=12,\n",
        "        n_layers=8,\n",
        "        d_ff=1536,\n",
        "        dropout=0.1,\n",
        "        learning_rate=3e-4,\n",
        "        batch_size=16,\n",
        "        num_epochs=100,\n",
        "        num_themes=20\n",
        "    )\n",
        "\n",
        "    generator = LinkedInPostGenerator(config)\n",
        "\n",
        "    logger.info(\"Preparing training data...\")\n",
        "    dataloader = generator.prepare_data()\n",
        "\n",
        "    logger.info(\"Starting training...\")\n",
        "    losses = generator.train(dataloader)\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(losses)\n",
        "    plt.title('Training Loss Over Time')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.grid(True)\n",
        "    plt.savefig('training_loss.png')\n",
        "    plt.show()\n",
        "\n",
        "    generator.load_model('best_linkedin_llm.pth')\n",
        "\n",
        "    logger.info(\"Generating sample posts...\")\n",
        "    themes = ['professional', 'motivational', 'industry_insights', 'personal_story', 'thought_leadership']\n",
        "\n",
        "    for theme in themes:\n",
        "        post = generator.generate_post(theme, max_length=100, temperature=0.7)\n",
        "        print(f\"\\n--- {theme.upper()} POST ---\")\n",
        "        print(post)\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config = ModelConfig()\n",
        "generator = LinkedInPostGenerator(config)\n",
        "generator.load_model('/content/best_linkedin_llm.pth')\n",
        "result = generator.generate_post('educational')\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L4tHub86U1Wj",
        "outputId": "0c9070e1-67c0-4d1b-a60c-4a89a46f5a1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "five principles for clean code : 1 write code for humans, not computers 2 use meaningful names 3 keep functions small and focused 4 comment the why, and you ' ll never struggle with behavioral questions again. #interviewtips #careeradvice #professionaldevelopment\n"
          ]
        }
      ]
    }
  ]
}